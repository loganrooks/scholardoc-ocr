# Milestone v2.1: Performance

**Status:** SHIPPED 2026-02-04
**Phases:** 11-14
**Total Plans:** 17

## Overview

Optimize Surya OCR performance on Apple Silicon through MPS acceleration, model caching, and cross-file batching. Expected cumulative improvement: 5-15x for multi-file batches.

## Phases

### Phase 11: Benchmarking Foundation & Metrics Fixes

**Goal:** Establish baseline performance measurements and fix timing/metadata bugs that would invalidate benchmarks.
**Depends on:** Nothing (first phase of v2.1)
**Requirements:** BENCH-01, BENCH-02, BENCH-03, BENCH-04, BENCH-05, BENCH-06, BENCH-07, BENCH-08
**Plans:** 5 plans

Plans:
- [x] 11-01-PLAN.md — Benchmark infrastructure (dependencies, timing module, fixtures)
- [x] 11-02-PLAN.md — Benchmark tests (model loading, inference, memory)
- [x] 11-03-PLAN.md — OCREngine.MIXED enum and compute_engine_from_pages
- [x] 11-04-PLAN.md — Pipeline fixes (Surya timing, engine field, quality re-eval)
- [x] 11-05-PLAN.md — CI workflow for regression detection

**Success Criteria (all verified):**
1. Running `pytest --benchmark-only` produces statistical performance data for model loading and OCR processing
2. Benchmark results include proper GPU synchronization (torch.mps.synchronize) for accurate MPS timing
3. Memory profiling with memray generates flame graphs showing allocation patterns
4. CI pipeline fails if performance regresses beyond threshold
5. Hardware-specific profiles (M1/M2/M3) can be selected for appropriate baselines
6. Surya model load time and inference time appear in phase_timings
7. Top-level engine field is "mixed" when some pages used Surya, "surya" when all did
8. Quality scores are re-evaluated after Surya processing (both scores preserved if fallback occurred)

### Phase 12: Device Configuration

**Goal:** Enable explicit MPS device selection with validation and fallback for Apple Silicon GPU acceleration.
**Depends on:** Phase 11 (need benchmarks to validate improvements)
**Requirements:** DEV-01, DEV-02, DEV-03, DEV-04
**Plans:** 5 plans

Plans:
- [x] 12-01-PLAN.md — Device detection infrastructure (DeviceType, DeviceInfo, detect_device)
- [x] 12-02-PLAN.md — FileResult device tracking (device_used field)
- [x] 12-03-PLAN.md — Surya and pipeline integration (use device detection, track device in results)
- [x] 12-04-PLAN.md — CLI --strict-gpu flag and startup validation (check_gpu_availability)
- [x] 12-05-PLAN.md — Inference-time GPU-to-CPU fallback (convert_pdf_with_fallback)

**Success Criteria (all verified):**
1. Pipeline explicitly uses MPS device on Apple Silicon (visible in logs: "Using device: mps")
2. Startup validates MPS availability and shows actionable error if unavailable
3. Processing automatically falls back to CPU when MPS fails mid-job
4. MPS bugs are handled via full-CPU fallback (detection/recognition split deferred - Marker API uses unified device)

### Phase 13: Model Caching

**Goal:** Eliminate repeated model loading overhead for MCP server by persisting loaded Surya models across requests.
**Depends on:** Phase 12 (need device configuration for correct model loading)
**Requirements:** MODEL-01, MODEL-02, MODEL-03, MODEL-04, MODEL-05
**Plans:** 3 plans

Plans:
- [x] 13-01-PLAN.md — ModelCache module (singleton, TTL, GPU cleanup utilities)
- [x] 13-02-PLAN.md — Pipeline integration (use cached models, inter-document cleanup)
- [x] 13-03-PLAN.md — MCP server integration (lifespan hooks, ocr_memory_stats tool)

**Success Criteria (all verified):**
1. Second MCP OCR request processes without 30-60s model loading delay
2. Cached models evict automatically after configurable TTL (default 30 minutes)
3. Memory cleanup between documents prevents accumulation (empty_cache + gc.collect)
4. MCP server startup can pre-load models when configured (warm pool)
5. Memory profiling shows VRAM usage during processing (accessible via API or logs)

### Phase 14: Cross-File Batching

**Goal:** Process all flagged pages across multiple files in a single Surya batch, maximizing GPU utilization.
**Depends on:** Phase 13 (need model caching for efficient batch processing)
**Requirements:** BATCH-01, BATCH-02, BATCH-03, BATCH-04, BATCH-05
**Plans:** 4 plans

Plans:
- [x] 14-01-PLAN.md — Batch configuration infrastructure (memory detection, env var setup, FlaggedPage)
- [x] 14-02-PLAN.md — Cross-file batching (combined PDF, single Surya call, result mapping)
- [x] 14-03-PLAN.md — Adaptive batch sizing (memory pressure monitoring, conservative defaults)
- [x] 14-04-PLAN.md — Gap closure: actual batch splitting when memory constrained

**Success Criteria (all verified):**
1. Processing 5 files with 10 flagged pages each produces one Surya batch of 50 pages (not 5 batches)
2. Batch size is configurable via environment variables (RECOGNITION_BATCH_SIZE, DETECTOR_BATCH_SIZE)
3. Default batch sizes adjust based on available memory (smaller on 8GB, larger on 32GB+)
4. Surya results map back correctly to source files (per-file, per-page)
5. Batch size adapts if memory pressure detected during processing

---

## Milestone Summary

**Key Decisions:**

- Benchmarking first: Establish baseline before any optimization changes
- Research-driven phase order: BENCH -> DEV -> MODEL -> BATCH (from research findings)
- Lazy torch imports: Avoid loading ML dependencies at module import time
- MPS sync for GPU timing: Always call mps_sync() before measuring GPU operation duration
- TTLCache maxsize=1: Only one model set cached at a time (Surya models too large for multiple)
- 30 minute default TTL: Balance between memory retention and reload frequency
- Single Surya call per batch: N files with flagged pages -> 1 Surya invocation
- Cleanup per batch not per file: GPU memory freed once after entire batch completes
- Multi-batch loop pattern: Split pages into sub-batches, process each with own Surya call

**Issues Resolved:**

- Inaccurate GPU timing (now synchronized with torch.mps.synchronize)
- Model loading 30-60s per request (now cached with TTL)
- Per-file Surya calls wasting GPU cycles (now cross-file batched)
- Memory pressure on 8GB machines (now adaptive batch sizing)

**Tech Debt:**

- psutil import at module level - requires `pip install -e '.[dev]'`
- Hardcoded "mps" device string in pipeline.py:402-403 - works on target platform

---

*For current project status, see .planning/ROADMAP.md*
