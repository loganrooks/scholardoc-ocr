---
phase: 14-cross-file-batching
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/scholardoc_ocr/batch.py
  - tests/test_batch.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Surya batch size environment variables are set before marker imports"
    - "Batch sizes scale based on available system memory"
    - "8GB machines get conservative defaults, 32GB+ get aggressive defaults"
  artifacts:
    - path: "src/scholardoc_ocr/batch.py"
      provides: "Batch configuration and memory detection"
      exports: ["configure_surya_batch_sizes", "get_available_memory_gb", "FlaggedPage"]
      min_lines: 80
    - path: "tests/test_batch.py"
      provides: "Unit tests for batch configuration"
      min_lines: 60
  key_links:
    - from: "src/scholardoc_ocr/batch.py"
      to: "os.environ"
      via: "setdefault for RECOGNITION_BATCH_SIZE, DETECTOR_BATCH_SIZE"
      pattern: "os\\.environ\\.setdefault"
    - from: "src/scholardoc_ocr/batch.py"
      to: "psutil.virtual_memory"
      via: "memory detection"
      pattern: "psutil\\.virtual_memory"
---

<objective>
Create batch configuration infrastructure for Surya batch sizing.

Purpose: BATCH-01, BATCH-02, BATCH-03 require hardware-aware batch size configuration. Surya batch sizes are controlled via environment variables that must be set before marker imports. This plan creates the batch.py module with memory detection, env var configuration, and the FlaggedPage dataclass for origin tracking.

Output: batch.py module with configure_surya_batch_sizes(), get_available_memory_gb(), FlaggedPage dataclass, and comprehensive unit tests.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/14-cross-file-batching/14-RESEARCH.md
@src/scholardoc_ocr/device.py
@src/scholardoc_ocr/model_cache.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create batch.py module with batch configuration</name>
  <files>src/scholardoc_ocr/batch.py</files>
  <action>
Create src/scholardoc_ocr/batch.py with:

1. **FlaggedPage dataclass** for tracking page origins:
   ```python
   @dataclass
   class FlaggedPage:
       """Track origin of a flagged page for result mapping."""
       file_result: FileResult  # Reference to source file result
       page_number: int         # 0-indexed page number in source
       input_path: Path         # Path to source PDF
       batch_index: int         # Position in combined batch PDF
   ```

2. **get_available_memory_gb() function**:
   - Use psutil.virtual_memory().total for system memory
   - For MPS, use system total (unified memory)
   - For CUDA, use torch.cuda.get_device_properties(0).total_memory
   - Lazy torch import (only if checking CUDA)
   - Return float in GB

3. **configure_surya_batch_sizes(device: str, available_memory_gb: float) -> dict[str, str]**:
   - Use os.environ.setdefault() (not direct assignment) to allow user overrides
   - Return dict of set values for logging
   - Batch size tiers (from research):
     - CPU: RECOGNITION=32, DETECTOR=6
     - GPU 8GB: RECOGNITION=32, DETECTOR=16
     - GPU 16GB: RECOGNITION=64, DETECTOR=32
     - GPU 32GB+: RECOGNITION=128, DETECTOR=64

4. **IMPORTANT**: All torch imports must be lazy (inside function body) to avoid loading ML deps at module import time. Follow pattern from device.py and model_cache.py.

5. Add module docstring explaining that env vars must be set before marker imports.

6. Import only TYPE_CHECKING for type hints (FileResult from types).
  </action>
  <verify>
    - `python -c "from scholardoc_ocr.batch import configure_surya_batch_sizes, get_available_memory_gb, FlaggedPage; print('imports ok')"` succeeds
    - `ruff check src/scholardoc_ocr/batch.py` passes
  </verify>
  <done>
    - batch.py exports configure_surya_batch_sizes, get_available_memory_gb, FlaggedPage
    - get_available_memory_gb returns system memory in GB
    - configure_surya_batch_sizes sets appropriate env vars based on memory tier
    - No torch imported at module level
  </done>
</task>

<task type="auto">
  <name>Task 2: Create unit tests for batch configuration</name>
  <files>tests/test_batch.py</files>
  <action>
Create tests/test_batch.py with:

1. **TestFlaggedPage class**:
   - test_flagged_page_creation: Verify dataclass fields
   - test_flagged_page_batch_index: Verify batch_index tracking

2. **TestGetAvailableMemory class**:
   - test_returns_positive_float: get_available_memory_gb() returns > 0.0
   - test_returns_reasonable_value: Returns value between 1.0 and 1024.0 (reasonable GB range)

3. **TestConfigureSuryaBatchSizes class**:
   - test_cpu_defaults: CPU gets conservative RECOGNITION=32, DETECTOR=6
   - test_mps_8gb: 8GB MPS gets RECOGNITION=32, DETECTOR=16
   - test_mps_16gb: 16GB MPS gets RECOGNITION=64, DETECTOR=32
   - test_mps_32gb: 32GB+ MPS gets RECOGNITION=128, DETECTOR=64
   - test_cuda_32gb: 32GB CUDA gets same as MPS 32GB
   - test_returns_dict_of_values: Returns dict with set values
   - test_preserves_existing_env_vars: Uses setdefault, doesn't override existing
   - test_env_var_override: If RECOGNITION_BATCH_SIZE already set, preserves it

Use pytest fixtures:
- @pytest.fixture(autouse=True) to clear env vars before each test
- Mock psutil.virtual_memory for memory tests
- Mock torch for CUDA memory tests

Follow existing test patterns from tests/test_model_cache.py.
  </action>
  <verify>
    - `pytest tests/test_batch.py -v` passes all tests
    - `ruff check tests/test_batch.py` passes
  </verify>
  <done>
    - All test cases pass
    - Tests cover all memory tiers (CPU, 8GB, 16GB, 32GB+)
    - Tests verify env var preservation (setdefault behavior)
    - Tests use proper mocking for psutil and torch
  </done>
</task>

</tasks>

<verification>
1. `ruff check src/scholardoc_ocr/batch.py tests/test_batch.py` - no lint errors
2. `pytest tests/test_batch.py -v` - all tests pass
3. `python -c "import scholardoc_ocr.batch"` - no torch loaded at import (verify with: `python -c "import sys; import scholardoc_ocr.batch; print('torch' not in sys.modules)"` returns True)
</verification>

<success_criteria>
- batch.py module exists with configure_surya_batch_sizes, get_available_memory_gb, FlaggedPage
- Memory detection works cross-platform (psutil for system, torch for GPU)
- Batch sizes scale appropriately: 8GB conservative, 32GB+ aggressive
- Environment variable override works (setdefault pattern)
- All unit tests pass
- No ML dependencies loaded at import time
</success_criteria>

<output>
After completion, create `.planning/phases/14-cross-file-batching/14-01-SUMMARY.md`
</output>
