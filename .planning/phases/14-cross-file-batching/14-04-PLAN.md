---
phase: 14-cross-file-batching
plan: 04
type: execute
wave: 1
depends_on: []
files_modified:
  - src/scholardoc_ocr/batch.py
  - src/scholardoc_ocr/pipeline.py
  - tests/test_batch.py
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "When memory pressure is detected, batch is split into smaller sub-batches"
    - "Each sub-batch is processed separately with its own Surya call"
    - "Results from all sub-batches are correctly mapped back to source files"
  artifacts:
    - path: "src/scholardoc_ocr/batch.py"
      provides: "split_into_batches() function for memory-aware batch splitting"
      exports: ["split_into_batches"]
    - path: "src/scholardoc_ocr/pipeline.py"
      provides: "Multi-batch processing loop when memory constrained"
      contains: "for sub_batch in batches:"
    - path: "tests/test_batch.py"
      provides: "Tests for batch splitting logic"
      contains: "TestSplitIntoBatches"
  key_links:
    - from: "src/scholardoc_ocr/pipeline.py"
      to: "batch.split_into_batches"
      via: "import and call when memory constrained"
      pattern: "split_into_batches\\("
    - from: "batch.split_into_batches"
      to: "compute_safe_batch_size"
      via: "uses safe batch size for splitting"
      pattern: "compute_safe_batch_size\\("
---

<objective>
Implement actual batch size adaptation when memory pressure is detected during processing.

Purpose: Close the gap in BATCH-05 where memory pressure is only logged but batches are not actually split. This ensures the pipeline doesn't freeze on memory-constrained 8GB machines when processing large cross-file batches.

Output: Working batch splitting that processes flagged pages in multiple smaller Surya calls when memory is constrained, with correct result mapping.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/14-cross-file-batching/14-03-SUMMARY.md

# Key source files
@src/scholardoc_ocr/batch.py
@src/scholardoc_ocr/pipeline.py
@tests/test_batch.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add batch splitting function to batch.py</name>
  <files>src/scholardoc_ocr/batch.py</files>
  <action>
Add a new function `split_into_batches()` that divides a list of FlaggedPage objects into smaller sub-batches based on available memory:

```python
def split_into_batches(
    flagged_pages: list[FlaggedPage],
    available_memory_gb: float,
    device: str,
) -> list[list[FlaggedPage]]:
    """Split flagged pages into memory-safe sub-batches.

    When memory is constrained, divides the pages into smaller batches
    that can be processed without freezing the system.

    Args:
        flagged_pages: All flagged pages to process.
        available_memory_gb: Available system memory in GB.
        device: Device type ("mps", "cuda", "cpu").

    Returns:
        List of sub-batches, each a list of FlaggedPage objects.
        If no splitting needed, returns single batch containing all pages.

    Examples:
        >>> pages = [FlaggedPage(...) for _ in range(50)]
        >>> batches = split_into_batches(pages, 4.0, "mps")
        >>> len(batches)  # Multiple small batches
        10
        >>> batches = split_into_batches(pages, 32.0, "mps")
        >>> len(batches)  # Single batch (enough memory)
        1
    """
```

Implementation requirements:
- Use `compute_safe_batch_size()` to determine max pages per sub-batch
- If all pages fit in one batch, return `[flagged_pages]` (single batch, no split)
- If splitting needed, divide pages into chunks of `safe_batch_size`
- Maintain batch_index values - each sub-batch's pages keep their original batch_index (for result mapping)
- Log at INFO level when splitting occurs: "Splitting {total} pages into {n} sub-batches of ~{size} pages"
  </action>
  <verify>
Run: `python -c "from scholardoc_ocr.batch import split_into_batches; print('Import OK')"`
Verify function exists and is importable.
  </verify>
  <done>
`split_into_batches()` is added to batch.py, correctly splits pages based on available memory, and logs when splitting occurs.
  </done>
</task>

<task type="auto">
  <name>Task 2: Integrate batch splitting into pipeline</name>
  <files>src/scholardoc_ocr/pipeline.py</files>
  <action>
Modify the Phase 2 Surya processing in `run_pipeline()` to use batch splitting when memory constrained:

1. Import `split_into_batches` from batch module (add to existing imports at line ~367)

2. Replace the single-batch processing (lines ~439-514) with multi-batch loop:

```python
# After check_memory_pressure (around line 406)
if is_constrained:
    safe_size = compute_safe_batch_size(total_flagged_pages, current_available, "mps")
    logger.warning(
        "Memory constrained (%.1fGB available). Splitting into sub-batches of ~%d pages",
        current_available,
        safe_size,
    )

# Collect all flagged pages
flagged_pages = collect_flagged_pages(flagged_results, input_paths)

# Split into batches based on memory (single batch if not constrained)
batches = split_into_batches(flagged_pages, current_available, device_used or "mps")
logger.info(
    "Cross-file batch: %d pages from %d files in %d sub-batch(es)",
    len(flagged_pages),
    len(flagged_results),
    len(batches),
)

# Process each sub-batch
total_inference_time = 0.0
for batch_idx, sub_batch in enumerate(batches):
    if not sub_batch:
        continue

    logger.info("Processing sub-batch %d/%d (%d pages)", batch_idx + 1, len(batches), len(sub_batch))

    # Create combined PDF for this sub-batch
    combined_pdf = config.output_dir / "work" / f"_surya_batch_{batch_idx}.pdf"
    create_combined_pdf(sub_batch, combined_pdf)

    # Surya call for this sub-batch
    t_inference = time.time()
    surya_markdown, fallback_occurred = surya.convert_pdf_with_fallback(
        combined_pdf,
        model_dict,
        config=surya_cfg,
        page_range=None,
        strict_gpu=config.strict_gpu,
    )
    mps_sync()
    batch_inference_time = time.time() - t_inference
    total_inference_time += batch_inference_time

    # Map results back for this sub-batch
    map_results_to_files(sub_batch, surya_markdown, analyzer)

    # Clean up this sub-batch's combined PDF
    if combined_pdf.exists():
        combined_pdf.unlink()

    # Clean up GPU memory between sub-batches (important for memory-constrained systems)
    if len(batches) > 1:
        cleanup_between_documents()

surya_inference_time = total_inference_time
```

3. Update the phase_timings to include sub-batch count:
```python
file_result.phase_timings["surya_sub_batches"] = len(batches)
```

Key requirements:
- Use `current_available` from check_memory_pressure for split decision
- Call `cleanup_between_documents()` between sub-batches (only if multiple batches)
- Accumulate total inference time across all sub-batches
- Each sub-batch gets its own combined PDF file (avoid overwriting)
  </action>
  <verify>
Run: `ruff check src/scholardoc_ocr/pipeline.py` - no errors
Run: `pytest tests/test_pipeline.py -v -k "test_" --tb=short -x` - existing tests still pass
  </verify>
  <done>
Pipeline processes flagged pages in multiple sub-batches when memory constrained, with cleanup between batches and correct result mapping.
  </done>
</task>

<task type="auto">
  <name>Task 3: Add tests for batch splitting</name>
  <files>tests/test_batch.py</files>
  <action>
Add a new test class `TestSplitIntoBatches` to test_batch.py with comprehensive tests:

```python
class TestSplitIntoBatches:
    """Tests for split_into_batches function."""

    def test_no_split_when_memory_sufficient(self):
        """Verify single batch returned when memory is plentiful."""
        # 32GB memory, 10 pages -> should fit in one batch

    def test_split_when_memory_constrained(self):
        """Verify pages split into multiple batches under memory pressure."""
        # 4GB memory, 50 pages -> should split into ~10 batches

    def test_split_preserves_batch_indices(self):
        """Verify original batch_index values preserved after splitting."""
        # Critical for result mapping

    def test_split_empty_pages(self):
        """Verify empty input returns empty list."""

    def test_split_single_page(self):
        """Verify single page always returns single batch."""

    def test_split_cpu_device(self):
        """Verify CPU uses different batch sizing (capped at 32)."""

    def test_split_logs_when_splitting(self, caplog):
        """Verify INFO log emitted when splitting occurs."""
```

Also add integration test in `TestBatchIntegration`:

```python
def test_constrained_memory_uses_sub_batches(self, mock_file_results, mock_input_paths, caplog):
    """Verify pipeline splits into sub-batches under memory pressure."""
    # Mock check_memory_pressure to return constrained
    # Verify multiple sub-batches created
```

Key requirements:
- Import `split_into_batches` from batch module
- Use mock FlaggedPage objects (similar to existing tests)
- Test both memory-sufficient and memory-constrained scenarios
- Verify batch_index preservation is critical for result mapping
  </action>
  <verify>
Run: `pytest tests/test_batch.py -v -k "TestSplitIntoBatches" --tb=short`
All new tests pass.
  </verify>
  <done>
At least 7 tests for `split_into_batches()` pass, covering sufficient memory, constrained memory, batch index preservation, edge cases, and logging.
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. **Unit tests pass:**
   ```bash
   pytest tests/test_batch.py -v --tb=short
   ```
   All 70+ tests pass (existing 66 + new ~7)

2. **Lint passes:**
   ```bash
   ruff check src/scholardoc_ocr/batch.py src/scholardoc_ocr/pipeline.py
   ```
   No errors

3. **Function chain works:**
   ```python
   from scholardoc_ocr.batch import (
       check_memory_pressure,
       compute_safe_batch_size,
       split_into_batches,
       FlaggedPage,
   )
   # Verify all functions importable and callable
   ```

4. **Behavior verification:**
   - With 32GB memory: single batch (no split)
   - With 4GB memory: multiple sub-batches
   - Batch indices preserved through split
   - GPU cleanup called between sub-batches
</verification>

<success_criteria>
- [ ] `split_into_batches()` function exists in batch.py
- [ ] Pipeline uses `split_into_batches()` when memory constrained
- [ ] GPU memory cleaned up between sub-batches
- [ ] Tests verify splitting behavior and batch index preservation
- [ ] All existing tests still pass
- [ ] BATCH-05 requirement fully satisfied: "Batch size adapts if memory pressure detected during processing"
</success_criteria>

<output>
After completion, create `.planning/phases/14-cross-file-batching/14-04-SUMMARY.md`
</output>
