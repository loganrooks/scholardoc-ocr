---
phase: 14-cross-file-batching
plan: 02
type: execute
wave: 2
depends_on: ["14-01"]
files_modified:
  - src/scholardoc_ocr/batch.py
  - src/scholardoc_ocr/pipeline.py
  - tests/test_batch.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "5 files with 10 flagged pages each produces one Surya batch of 50 pages"
    - "Surya results map back correctly to each source file and page"
    - "Combined PDF is a temporary file that gets cleaned up"
  artifacts:
    - path: "src/scholardoc_ocr/batch.py"
      provides: "Cross-file batching functions"
      exports: ["collect_flagged_pages", "create_combined_pdf", "map_results_to_files"]
      min_lines: 150
    - path: "src/scholardoc_ocr/pipeline.py"
      provides: "Single Surya call for all flagged pages"
      contains: "collect_flagged_pages"
  key_links:
    - from: "src/scholardoc_ocr/pipeline.py"
      to: "src/scholardoc_ocr/batch.py"
      via: "import and call collect_flagged_pages, create_combined_pdf"
      pattern: "from .batch import"
    - from: "src/scholardoc_ocr/batch.py"
      to: "fitz"
      via: "PyMuPDF for combined PDF creation"
      pattern: "fitz\\.open"
---

<objective>
Implement cross-file page batching for Surya OCR.

Purpose: BATCH-04 requires aggregating flagged pages across all files into a single Surya call. The current pipeline processes files sequentially, resulting in N Surya calls for N files. This plan implements page collection, combined PDF creation, single batch processing, and result mapping back to source files.

Output: Extended batch.py with cross-file batching functions, updated pipeline.py with single Surya call pattern, and integration tests.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/14-cross-file-batching/14-RESEARCH.md
@.planning/phases/14-cross-file-batching/14-01-SUMMARY.md
@src/scholardoc_ocr/pipeline.py
@src/scholardoc_ocr/batch.py
@src/scholardoc_ocr/processor.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add cross-file batching functions to batch.py</name>
  <files>src/scholardoc_ocr/batch.py</files>
  <action>
Extend src/scholardoc_ocr/batch.py with:

1. **collect_flagged_pages(file_results: list[FileResult], input_paths: dict[str, Path]) -> list[FlaggedPage]**:
   - Aggregate flagged pages from all file results
   - input_paths maps filename to input Path
   - Assign batch_index sequentially (0, 1, 2, ...)
   - Return list ordered for combined PDF creation
   - Example:
     ```python
     pages = []
     for fr in file_results:
         input_path = input_paths[fr.filename]
         for page in fr.flagged_pages:
             pages.append(FlaggedPage(
                 file_result=fr,
                 page_number=page.page_number,
                 input_path=input_path,
                 batch_index=len(pages),
             ))
     return pages
     ```

2. **create_combined_pdf(flagged_pages: list[FlaggedPage], output_path: Path) -> None**:
   - Use PyMuPDF (fitz) to create combined PDF
   - For each FlaggedPage, extract single page and insert into result
   - Must match batch_index order exactly
   - Example:
     ```python
     result_doc = fitz.open()
     for page in flagged_pages:
         with fitz.open(page.input_path) as source:
             result_doc.insert_pdf(
                 source,
                 from_page=page.page_number,
                 to_page=page.page_number,
             )
     result_doc.save(output_path)
     result_doc.close()
     ```

3. **map_results_to_files(flagged_pages: list[FlaggedPage], surya_text: str, analyzer: QualityAnalyzer) -> None**:
   - Parse surya_text to extract per-page content
   - Update each FlaggedPage's file_result.pages[page_number] with:
     - engine = OCREngine.SURYA
     - quality_score from analyzer.analyze()
     - flagged based on quality threshold
     - status = GOOD or FLAGGED
   - NOTE: Surya returns single markdown string. For now, assume entire text replaces all flagged pages. Plan 03 will refine page-level splitting if needed.
   - Mutates file_result.pages in place.

Add import for fitz (already a dependency).
  </action>
  <verify>
    - `python -c "from scholardoc_ocr.batch import collect_flagged_pages, create_combined_pdf, map_results_to_files; print('imports ok')"` succeeds
    - `ruff check src/scholardoc_ocr/batch.py` passes
  </verify>
  <done>
    - batch.py exports collect_flagged_pages, create_combined_pdf, map_results_to_files
    - collect_flagged_pages correctly assigns batch_index
    - create_combined_pdf uses PyMuPDF to combine pages
    - map_results_to_files updates source file results with Surya output
  </done>
</task>

<task type="auto">
  <name>Task 2: Update pipeline.py to use single Surya batch</name>
  <files>src/scholardoc_ocr/pipeline.py</files>
  <action>
Modify Phase 2 (Surya) in src/scholardoc_ocr/pipeline.py:

1. **Before Surya phase, configure batch sizes**:
   At the start of the "if flagged_results:" block, before model loading:
   ```python
   from .batch import configure_surya_batch_sizes, get_available_memory_gb

   # Configure Surya batch sizes based on hardware (BATCH-02, BATCH-03)
   available_mem = get_available_memory_gb()
   batch_config = configure_surya_batch_sizes(device_used, available_mem)
   logger.info("Surya batch config: %s (memory: %.1fGB)", batch_config, available_mem)
   ```
   IMPORTANT: This must happen BEFORE the ModelCache.get_models() call which triggers marker imports.

2. **Replace per-file Surya loop with single batch**:
   Replace the current `for file_result in flagged_results:` loop with:

   a) Collect all flagged pages:
   ```python
   from .batch import collect_flagged_pages, create_combined_pdf, map_results_to_files

   # Build input_paths mapping
   input_paths = {p.name: p for p in input_files}

   # Collect all flagged pages across all files (BATCH-04)
   flagged_pages = collect_flagged_pages(flagged_results, input_paths)
   logger.info("Cross-file batch: %d pages from %d files", len(flagged_pages), len(flagged_results))
   ```

   b) Create combined PDF in work directory:
   ```python
   combined_pdf = config.output_dir / "work" / "_surya_batch.pdf"
   create_combined_pdf(flagged_pages, combined_pdf)
   ```

   c) Single Surya call on combined PDF:
   ```python
   t_inference = time.time()
   surya_markdown, fallback_occurred = surya.convert_pdf_with_fallback(
       combined_pdf,
       model_dict,
       config=surya_cfg,
       page_range=None,  # Process all pages in combined PDF
       strict_gpu=config.strict_gpu,
   )
   mps_sync()
   surya_inference_time = time.time() - t_inference
   ```

   d) Map results back to source files:
   ```python
   from .quality import QualityAnalyzer
   analyzer = QualityAnalyzer(config.quality_threshold, max_samples=config.max_samples)
   map_results_to_files(flagged_pages, surya_markdown, analyzer)
   ```

   e) Update file_result metadata (device_used, phase_timings):
   ```python
   for file_result in flagged_results:
       file_result.device_used = device_used
       file_result.phase_timings["surya_inference"] = surya_inference_time
       file_result.phase_timings["surya_model_load"] = surya_model_load_time
       if fallback_occurred:
           file_result.device_used = "cpu"
           file_result.phase_timings["surya_fallback"] = True
   ```

   f) Clean up combined PDF:
   ```python
   if combined_pdf.exists():
       combined_pdf.unlink()
   ```

   g) Keep the cleanup_between_documents() call after processing.

3. **Update .txt file writing**:
   After map_results_to_files, update the text files for each file:
   ```python
   for file_result in flagged_results:
       # Get Surya-enhanced text for this file's flagged pages
       text_path = config.output_dir / "final" / f"{Path(file_result.filename).stem}.txt"
       if text_path.exists():
           # Read existing text, update flagged pages
           existing_text = text_path.read_text(encoding="utf-8")
           page_texts = existing_text.split("\n\n")
           for page in file_result.pages:
               if page.engine == OCREngine.SURYA and page.text:
                   if page.page_number < len(page_texts):
                       page_texts[page.page_number] = page.text
           text_path.write_text(_postprocess("\n\n".join(page_texts)), encoding="utf-8")
   ```

4. **Keep callback progress updates**:
   Update progress after batch completes, not per-file.
  </action>
  <verify>
    - `ruff check src/scholardoc_ocr/pipeline.py` passes
    - `pytest tests/test_pipeline.py -v` passes
  </verify>
  <done>
    - Pipeline uses single Surya call for all flagged pages across files
    - Batch sizes configured before model loading (BATCH-02)
    - Memory-aware defaults applied (BATCH-03)
    - Results mapped back to correct source files
    - Combined PDF cleaned up after processing
    - Existing tests still pass
  </done>
</task>

<task type="auto">
  <name>Task 3: Add integration tests for cross-file batching</name>
  <files>tests/test_batch.py</files>
  <action>
Add to tests/test_batch.py:

1. **TestCollectFlaggedPages class**:
   - test_collects_from_multiple_files: 2 files with 3 flagged pages each -> 6 FlaggedPage objects
   - test_batch_index_sequential: batch_index 0,1,2,3,4,5 for 6 pages
   - test_empty_file_results: Empty list returns empty list
   - test_no_flagged_pages: Files with no flagged pages returns empty list

2. **TestCreateCombinedPdf class**:
   - test_creates_combined_pdf: Creates valid PDF with correct page count
   - test_page_order_matches_batch_index: Pages in combined PDF match batch_index order
   - Use tmp_path fixture for output
   - Create small test PDFs using fitz

3. **TestMapResultsToFiles class**:
   - test_updates_engine_to_surya: All flagged pages get engine=SURYA
   - test_updates_quality_scores: Quality scores updated from analyzer
   - test_mutates_file_result_in_place: Original FileResult modified
   - Mock QualityAnalyzer

Use existing test helpers and fixtures where available.
  </action>
  <verify>
    - `pytest tests/test_batch.py -v` passes all tests
    - `ruff check tests/test_batch.py` passes
  </verify>
  <done>
    - Tests verify collect_flagged_pages aggregates correctly
    - Tests verify create_combined_pdf creates valid multi-page PDF
    - Tests verify map_results_to_files updates source file results
    - All batch.py tests pass
  </done>
</task>

</tasks>

<verification>
1. `ruff check src/scholardoc_ocr/batch.py src/scholardoc_ocr/pipeline.py tests/test_batch.py` - no lint errors
2. `pytest tests/test_batch.py tests/test_pipeline.py -v` - all tests pass
3. Manual verification: Create 2 test PDFs, run pipeline with force_surya, verify single Surya call in logs
</verification>

<success_criteria>
- Cross-file batching works: N files with flagged pages -> 1 Surya call
- Batch sizes configured before marker imports
- Results correctly mapped back to source files
- Combined PDF created and cleaned up
- All existing tests pass
- New integration tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/14-cross-file-batching/14-02-SUMMARY.md`
</output>
