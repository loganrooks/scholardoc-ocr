---
phase: 14-cross-file-batching
plan: 02
type: execute
wave: 2
depends_on: ["14-01"]
files_modified:
  - src/scholardoc_ocr/batch.py
  - src/scholardoc_ocr/pipeline.py
  - tests/test_batch.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "5 files with 10 flagged pages each produces one Surya batch of 50 pages"
    - "Surya results map back to source files with best-effort per-page splitting"
    - "Combined PDF is a temporary file that gets cleaned up"
  artifacts:
    - path: "src/scholardoc_ocr/batch.py"
      provides: "Cross-file batching functions"
      exports: ["collect_flagged_pages", "create_combined_pdf", "map_results_to_files", "split_markdown_by_pages"]
      min_lines: 180
    - path: "src/scholardoc_ocr/pipeline.py"
      provides: "Single Surya call for all flagged pages"
      contains: "collect_flagged_pages"
  key_links:
    - from: "src/scholardoc_ocr/pipeline.py"
      to: "src/scholardoc_ocr/batch.py"
      via: "import and call collect_flagged_pages, create_combined_pdf"
      pattern: "from .batch import"
    - from: "src/scholardoc_ocr/batch.py"
      to: "fitz"
      via: "PyMuPDF for combined PDF creation"
      pattern: "fitz\\.open"
---

<objective>
Implement cross-file page batching for Surya OCR.

Purpose: BATCH-04 requires aggregating flagged pages across all files into a single Surya call. The current pipeline processes files sequentially, resulting in N Surya calls for N files. This plan implements page collection, combined PDF creation, single batch processing, and result mapping back to source files.

Output: Extended batch.py with cross-file batching functions, updated pipeline.py with single Surya call pattern, and integration tests.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/14-cross-file-batching/14-RESEARCH.md
@.planning/phases/14-cross-file-batching/14-01-SUMMARY.md
@src/scholardoc_ocr/pipeline.py
@src/scholardoc_ocr/batch.py
@src/scholardoc_ocr/processor.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add cross-file batching functions to batch.py</name>
  <files>src/scholardoc_ocr/batch.py</files>
  <action>
Extend src/scholardoc_ocr/batch.py with:

1. **collect_flagged_pages(file_results: list[FileResult], input_paths: dict[str, Path]) -> list[FlaggedPage]**:
   - Aggregate flagged pages from all file results
   - input_paths maps filename to input Path
   - Assign batch_index sequentially (0, 1, 2, ...)
   - Return list ordered for combined PDF creation
   - Example:
     ```python
     pages = []
     for fr in file_results:
         input_path = input_paths[fr.filename]
         for page in fr.flagged_pages:
             pages.append(FlaggedPage(
                 file_result=fr,
                 page_number=page.page_number,
                 input_path=input_path,
                 batch_index=len(pages),
             ))
     return pages
     ```

2. **create_combined_pdf(flagged_pages: list[FlaggedPage], output_path: Path) -> None**:
   - Use PyMuPDF (fitz) to create combined PDF
   - For each FlaggedPage, extract single page and insert into result
   - Must match batch_index order exactly
   - Example:
     ```python
     result_doc = fitz.open()
     for page in flagged_pages:
         with fitz.open(page.input_path) as source:
             result_doc.insert_pdf(
                 source,
                 from_page=page.page_number,
                 to_page=page.page_number,
             )
     result_doc.save(output_path)
     result_doc.close()
     ```

3. **split_markdown_by_pages(markdown: str, page_count: int) -> list[str]**:
   - Parse Surya markdown to extract per-page text using heuristics
   - Primary: Split on horizontal rules `\n---\n` (Marker often inserts these)
   - Secondary: Split on triple newlines `\n\n\n` (page break heuristic)
   - Fallback: If fewer splits than pages, distribute text evenly OR assign all to first page
   - Always returns exactly `page_count` strings (may be empty for some)
   - Example:
     ```python
     def split_markdown_by_pages(markdown: str, page_count: int) -> list[str]:
         """Split Surya markdown into per-page text.

         Uses heuristics since Marker doesn't provide explicit page markers.
         Falls back to assigning all text to first page if splitting fails.
         """
         if page_count == 0:
             return []
         if page_count == 1:
             return [markdown]

         # Try horizontal rule splits first
         parts = re.split(r'\n-{3,}\n', markdown)
         if len(parts) >= page_count:
             return parts[:page_count]

         # Try triple newline splits
         parts = re.split(r'\n{3,}', markdown)
         if len(parts) >= page_count:
             return parts[:page_count]

         # Fallback: first page gets all text, rest empty
         result = [markdown] + [''] * (page_count - 1)
         return result
     ```

4. **map_results_to_files(flagged_pages: list[FlaggedPage], surya_text: str, analyzer: QualityAnalyzer) -> None**:
   - Call split_markdown_by_pages to get per-page text
   - For each FlaggedPage at batch_index i:
     - Get page_text = page_texts[i]
     - Update file_result.pages[page_number] with:
       - text = page_text
       - engine = OCREngine.SURYA
       - quality_score = analyzer.analyze(page_text).score
       - flagged = quality_score < analyzer.threshold
       - status = GOOD if not flagged else FLAGGED
   - Mutates file_result.pages in place
   - Example:
     ```python
     def map_results_to_files(
         flagged_pages: list[FlaggedPage],
         surya_text: str,
         analyzer: QualityAnalyzer,
     ) -> None:
         page_texts = split_markdown_by_pages(surya_text, len(flagged_pages))

         for fp in flagged_pages:
             text = page_texts[fp.batch_index]
             result = analyzer.analyze(text)

             # Update the PageResult in the source FileResult
             page_result = fp.file_result.pages[fp.page_number]
             page_result.text = text
             page_result.engine = OCREngine.SURYA
             page_result.quality_score = result.score
             page_result.flagged = result.score < analyzer.threshold
             page_result.status = PageStatus.GOOD if not page_result.flagged else PageStatus.FLAGGED
     ```

5. Add imports: `import re` for regex splitting, import OCREngine and PageStatus from types.

Add import for fitz (already a dependency).
  </action>
  <verify>
    - `python -c "from scholardoc_ocr.batch import collect_flagged_pages, create_combined_pdf, map_results_to_files, split_markdown_by_pages; print('imports ok')"` succeeds
    - `ruff check src/scholardoc_ocr/batch.py` passes
  </verify>
  <done>
    - batch.py exports collect_flagged_pages, create_combined_pdf, map_results_to_files, split_markdown_by_pages
    - collect_flagged_pages correctly assigns batch_index
    - create_combined_pdf uses PyMuPDF to combine pages
    - split_markdown_by_pages splits markdown with heuristics, always returns correct count
    - map_results_to_files updates source file results with per-page Surya output and quality scores
  </done>
</task>

<task type="auto">
  <name>Task 2: Update pipeline.py to use single Surya batch</name>
  <files>src/scholardoc_ocr/pipeline.py</files>
  <action>
Modify Phase 2 (Surya) in src/scholardoc_ocr/pipeline.py:

**Part A: Configure batch sizes (before model loading)**

At the start of the "if flagged_results:" block, before model loading:
```python
from .batch import configure_surya_batch_sizes, get_available_memory_gb

# Configure Surya batch sizes based on hardware (BATCH-02, BATCH-03)
available_mem = get_available_memory_gb()
batch_config = configure_surya_batch_sizes(device_used, available_mem)
logger.info("Surya batch config: %s (memory: %.1fGB)", batch_config, available_mem)
```
IMPORTANT: This must happen BEFORE the ModelCache.get_models() call which triggers marker imports.

**Part B: Collect all flagged pages**

Replace the current `for file_result in flagged_results:` loop with page collection:
```python
from .batch import collect_flagged_pages, create_combined_pdf, map_results_to_files

# Build input_paths mapping
input_paths = {p.name: p for p in input_files}

# Collect all flagged pages across all files (BATCH-04)
flagged_pages = collect_flagged_pages(flagged_results, input_paths)
logger.info("Cross-file batch: %d pages from %d files", len(flagged_pages), len(flagged_results))
```

**Part C: Create combined PDF and process**

```python
# Create combined PDF in work directory
combined_pdf = config.output_dir / "work" / "_surya_batch.pdf"
create_combined_pdf(flagged_pages, combined_pdf)

# Single Surya call on combined PDF
t_inference = time.time()
surya_markdown, fallback_occurred = surya.convert_pdf_with_fallback(
    combined_pdf,
    model_dict,
    config=surya_cfg,
    page_range=None,  # Process all pages in combined PDF
    strict_gpu=config.strict_gpu,
)
mps_sync()
surya_inference_time = time.time() - t_inference
```

**Part D: Map results back to source files**

```python
from .quality import QualityAnalyzer
analyzer = QualityAnalyzer(config.quality_threshold, max_samples=config.max_samples)
map_results_to_files(flagged_pages, surya_markdown, analyzer)
```

**Part E: Update file_result metadata**

```python
for file_result in flagged_results:
    file_result.device_used = device_used
    file_result.phase_timings["surya_inference"] = surya_inference_time
    file_result.phase_timings["surya_model_load"] = surya_model_load_time
    if fallback_occurred:
        file_result.device_used = "cpu"
        file_result.phase_timings["surya_fallback"] = True
```

**Part F: Update .txt files and cleanup**

```python
# Update text files with Surya-enhanced text
for file_result in flagged_results:
    text_path = config.output_dir / "final" / f"{Path(file_result.filename).stem}.txt"
    if text_path.exists():
        existing_text = text_path.read_text(encoding="utf-8")
        page_texts = existing_text.split("\n\n")
        for page in file_result.pages:
            if page.engine == OCREngine.SURYA and page.text:
                if page.page_number < len(page_texts):
                    page_texts[page.page_number] = page.text
        text_path.write_text(_postprocess("\n\n".join(page_texts)), encoding="utf-8")

# Clean up combined PDF
if combined_pdf.exists():
    combined_pdf.unlink()
```

**Part G: Keep cleanup and progress**

Keep the cleanup_between_documents() call after processing.
Update progress after batch completes, not per-file.
  </action>
  <verify>
    - `ruff check src/scholardoc_ocr/pipeline.py` passes
    - `pytest tests/test_pipeline.py -v` passes
  </verify>
  <done>
    - Pipeline uses single Surya call for all flagged pages across files
    - Batch sizes configured before model loading (BATCH-02)
    - Memory-aware defaults applied (BATCH-03)
    - Results correctly mapped back to source files with per-page text
    - Combined PDF created and cleaned up (verified by file deletion)
    - Existing tests still pass
  </done>
</task>

<task type="auto">
  <name>Task 3: Add integration tests for cross-file batching</name>
  <files>tests/test_batch.py</files>
  <action>
Add to tests/test_batch.py:

1. **TestSplitMarkdownByPages class**:
   - test_single_page: 1 page returns [markdown]
   - test_empty_input: 0 pages returns []
   - test_horizontal_rule_split: "page1\n---\npage2\n---\npage3" with 3 pages -> ["page1", "page2", "page3"]
   - test_triple_newline_split: "page1\n\n\npage2\n\n\npage3" with 3 pages -> ["page1", "page2", "page3"]
   - test_fallback_first_page: "no separators" with 3 pages -> ["no separators", "", ""]
   - test_extra_splits_truncated: "a\n---\nb\n---\nc\n---\nd" with 2 pages -> ["a", "b"]

2. **TestCollectFlaggedPages class**:
   - test_collects_from_multiple_files: 2 files with 3 flagged pages each -> 6 FlaggedPage objects
   - test_batch_index_sequential: batch_index 0,1,2,3,4,5 for 6 pages
   - test_empty_file_results: Empty list returns empty list
   - test_no_flagged_pages: Files with no flagged pages returns empty list

3. **TestCreateCombinedPdf class**:
   - test_creates_combined_pdf: Creates valid PDF with correct page count
   - test_page_order_matches_batch_index: Pages in combined PDF match batch_index order
   - Use tmp_path fixture for output
   - Create small test PDFs using fitz

4. **TestMapResultsToFiles class**:
   - test_updates_engine_to_surya: All flagged pages get engine=SURYA
   - test_updates_quality_scores: Quality scores updated from analyzer
   - test_assigns_text_per_page: Each FlaggedPage gets its split text assigned
   - test_mutates_file_result_in_place: Original FileResult modified
   - Mock QualityAnalyzer to return predictable scores

Use existing test helpers and fixtures where available.
  </action>
  <verify>
    - `pytest tests/test_batch.py -v` passes all tests
    - `ruff check tests/test_batch.py` passes
  </verify>
  <done>
    - Tests verify split_markdown_by_pages handles all cases (rules, newlines, fallback)
    - Tests verify collect_flagged_pages aggregates correctly
    - Tests verify create_combined_pdf creates valid multi-page PDF
    - Tests verify map_results_to_files updates source file results with per-page text
    - All batch.py tests pass
  </done>
</task>

</tasks>

<verification>
1. `ruff check src/scholardoc_ocr/batch.py src/scholardoc_ocr/pipeline.py tests/test_batch.py` - no lint errors
2. `pytest tests/test_batch.py tests/test_pipeline.py -v` - all tests pass
3. Manual verification: Create 2 test PDFs, run pipeline with force_surya, verify single Surya call in logs
</verification>

<success_criteria>
- Cross-file batching works: N files with flagged pages -> 1 Surya call
- Batch sizes configured before marker imports
- Per-page text splitting implemented with heuristics and fallback
- Results correctly mapped back to source files with text assigned to PageResult.text
- Combined PDF created and cleaned up
- All existing tests pass
- New integration tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/14-cross-file-batching/14-02-SUMMARY.md`
</output>
