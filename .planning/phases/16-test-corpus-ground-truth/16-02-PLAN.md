---
phase: 16-test-corpus-ground-truth
plan: 02
type: execute
wave: 2
depends_on: ["16-01"]
files_modified:
  - tests/corpus/corpus.json
autonomous: false

must_haves:
  truths:
    - "PDF symlinks exist in tests/corpus/pdfs/ pointing to all 4 philosophy documents"
    - "Diagnostic baseline data (.diagnostics.json, .txt, .json) exists for all 4 documents in tests/corpus/baselines/"
    - "Coverage matrix analysis has identified pages spanning all detected struggle categories"
    - "Page selection (difficult + regression) has been determined and recorded"
    - "Selected pages have been rendered as 300 DPI PNGs for Opus transcription"
  artifacts:
    - path: "tests/corpus/pdfs/simondon-technical-objects.pdf"
      provides: "Symlink to Simondon PDF"
    - path: "tests/corpus/baselines/simondon-technical-objects/final/simondon-technical-objects.diagnostics.json"
      provides: "Diagnostic sidecar for Simondon baseline"
      contains: "struggle_categories"
    - path: "tests/corpus/page_selection.json"
      provides: "Machine-readable page selection with difficult and regression sets"
      contains: "difficult"
    - path: "tests/corpus/images/"
      provides: "Rendered page PNGs for Opus transcription"
  key_links:
    - from: "scripts/corpus/build_coverage_matrix.py"
      to: "tests/corpus/baselines/**/final/*.diagnostics.json"
      via: "parses diagnostic sidecar to build coverage matrix"
      pattern: "struggle_categories"
    - from: "tests/corpus/corpus.json"
      to: "tests/corpus/baselines/"
      via: "baseline paths in manifest match actual baseline locations"
      pattern: "baselines/"
    - from: "tests/corpus/page_selection.json"
      to: "tests/corpus/images/"
      via: "selected pages rendered as PNGs for transcription"
---

<objective>
Capture diagnostic baselines for all 4 corpus documents and use coverage matrix analysis to select pages for ground truth creation.

Purpose: The baseline diagnostic data drives intelligent page selection (coverage-based, not count-based). Without baselines, ground truth page selection would be arbitrary. This plan transforms the empty corpus infrastructure into a data-informed page selection.

Output: Baseline diagnostic data for all 4 documents, coverage matrix analysis, page selection recommendation, rendered page images ready for Opus transcription.
</objective>

<execution_context>
@/Users/rookslog/.claude/get-shit-done/workflows/execute-plan.md
@/Users/rookslog/.claude/get-shit-done/templates/summary-standard.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/16-test-corpus-ground-truth/16-CONTEXT.md
@.planning/phases/16-test-corpus-ground-truth/16-RESEARCH.md
@.planning/phases/16-test-corpus-ground-truth/16-01-SUMMARY.md
</context>

<tasks>

<task type="checkpoint:human-action" gate="blocking">
  <name>Task 1: Create PDF symlinks and run diagnostic baselines</name>
  <action>
The user must perform these steps manually because they require knowledge of local PDF file paths and long-running pipeline execution (15-30 min per document).

**Step 1: Create PDF symlinks**

Create symbolic links in `tests/corpus/pdfs/` pointing to the actual PDF files on your system:

```bash
cd tests/corpus/pdfs/

# Adjust paths to match your actual PDF locations
ln -s /path/to/simondon-du-mode-dexistence.pdf simondon-technical-objects.pdf
ln -s /path/to/derrida-of-grammatology.pdf derrida-grammatology.pdf
ln -s /path/to/derrida-margins-of-philosophy.pdf derrida-margins.pdf
ln -s /path/to/derrida-dissemination.pdf derrida-dissemination.pdf
```

Verify symlinks resolve: `ls -la tests/corpus/pdfs/`

**Step 2: Run diagnostic baselines for each document**

Run the pipeline with `--diagnostics --extract-text --force` for each document. Process one at a time (15-30 min each). The output directory must match the manifest's baseline path.

```bash
# From project root
# Document 1: Simondon
ocr --diagnostics --extract-text --force \
    -o tests/corpus/baselines/simondon-technical-objects \
    tests/corpus/pdfs/simondon-technical-objects.pdf

# Document 2: Derrida - Of Grammatology
ocr --diagnostics --extract-text --force \
    -o tests/corpus/baselines/derrida-grammatology \
    tests/corpus/pdfs/derrida-grammatology.pdf

# Document 3: Derrida - Margins of Philosophy
ocr --diagnostics --extract-text --force \
    -o tests/corpus/baselines/derrida-margins \
    tests/corpus/pdfs/derrida-margins.pdf

# Document 4: Derrida - Dissemination
ocr --diagnostics --extract-text --force \
    -o tests/corpus/baselines/derrida-dissemination \
    tests/corpus/pdfs/derrida-dissemination.pdf
```

CRITICAL: Use `--extract-text` to preserve .txt output files. Without it, the pipeline deletes .txt files at cleanup.

**Step 3: Verify baseline output**

Each baseline directory should contain in its `final/` subdirectory:
- `{document-id}.diagnostics.json` -- per-page diagnostic data (committed)
- `{document-id}.txt` -- OCR output text (committed)
- `{document-id}.json` -- pipeline metadata (committed)
- `{document-id}.pdf` -- OCR'd PDF (gitignored, large binary)

```bash
# Quick verification
for doc in simondon-technical-objects derrida-grammatology derrida-margins derrida-dissemination; do
    echo "=== $doc ==="
    ls tests/corpus/baselines/$doc/final/
done
```
  </action>
  <resume-signal>Type "baselines complete" when all 4 documents have been processed and verified. If any document failed, describe the issue.</resume-signal>
</task>

<task type="auto">
  <name>Task 2: Run coverage analysis, select pages, render PNGs, and update manifest</name>
  <files>
    tests/corpus/corpus.json
    tests/corpus/page_selection.json
  </files>
  <action>
After the user confirms baselines are complete, perform these steps:

**Step 1: Update manifest with actual page counts**

Read each `.diagnostics.json` sidecar to get the actual page count for each document. Update `corpus.json` `page_count` fields with real values.

**Step 2: Run coverage matrix analysis**

```bash
python scripts/corpus/build_coverage_matrix.py
```

This produces:
- Human-readable coverage report on stdout
- `tests/corpus/page_selection.json` with recommended page selection

Review the coverage report. The selection should have:
- **Difficult pages (~40-50):** At least 2 pages per detected struggle category, all gray_zone pages, all signal_disagreement pages
- **Regression pages (~15-20):** Per document: 1 ToC/index page (if exists), 1 front matter page, 2-3 clean body text pages (quality_score > 0.90, no struggle categories), 1 bibliography page (if exists)

If the automated selection is insufficient (e.g., a struggle category has only 1 example), manually adjust `page_selection.json` to improve coverage. Use judgment per research discretion recommendations: minimum 2 pages per category, all gray_zone, all signal_disagreement. If a category has 10+ pages, sample 3-4 spanning different documents.

**Step 3: Render selected pages as PNGs**

Render all selected pages (both difficult and regression) at 300 DPI for Opus transcription:

```bash
# Render pages for each document using the selection
# Read page_selection.json to get page numbers per document, then:
python scripts/corpus/render_pages.py simondon-technical-objects <page_numbers_from_selection>
python scripts/corpus/render_pages.py derrida-grammatology <page_numbers_from_selection>
python scripts/corpus/render_pages.py derrida-margins <page_numbers_from_selection>
python scripts/corpus/render_pages.py derrida-dissemination <page_numbers_from_selection>
```

**Step 4: Update manifest with diagnostic summary**

For each document in `corpus.json`, populate the `diagnostic_summary` field from the baseline data:
- `total_pages`: number of pages in the document
- `flagged_pages`: number of pages with `flagged: true`
- `flagged_percentage`: flagged_pages / total_pages
- `dominant_struggle_categories`: list of struggle categories that appear most frequently
- `quality_score_range`: `[min_score, max_score]` across all pages
- `signal_disagreement_count`: number of pages with `has_signal_disagreement: true`

Also update `baseline.run_date` with the current date and `baseline.pipeline_version` with "0.2.0".

**Step 5: Record page selection in manifest**

Update each document's entry in `corpus.json` to include a `selected_pages` field (separate from `ground_truth_pages` which gets populated in Plan 03):
```json
"selected_pages": {
    "difficult": [42, 73, 150],
    "regression": [0, 1, 200]
}
```

This records the selection rationale. `ground_truth_pages` will be populated in Plan 03 after transcription.
  </action>
  <verify>
- `python -m json.tool tests/corpus/corpus.json` validates JSON
- `python -m json.tool tests/corpus/page_selection.json` validates JSON
- `corpus.json` has non-null `page_count` for all 4 documents
- `corpus.json` has non-null `diagnostic_summary` for all 4 documents
- `corpus.json` has `selected_pages` with both `difficult` and `regression` arrays for each document
- `corpus.json` has `baseline.run_date` and `baseline.pipeline_version` populated
- `ls tests/corpus/images/*/` shows PNG files for selected pages
- Page selection covers all struggle categories detected in the baselines
  </verify>
  <done>
Coverage matrix analysis complete. Page selection determined (difficult + regression sets) based on diagnostic data. Selected pages rendered as 300 DPI PNGs. Manifest updated with page counts, diagnostic summaries, baseline metadata, and page selections. Corpus is ready for ground truth creation.
  </done>
</task>

</tasks>

<verification>
1. All 4 documents have baseline output in tests/corpus/baselines/{doc-id}/final/ with .diagnostics.json, .txt, and .json files
2. Coverage matrix analysis successfully parsed all diagnostic sidecars
3. Page selection covers all detected struggle categories with at least 2 examples each
4. All selected pages have been rendered as PNGs in tests/corpus/images/{doc-id}/
5. corpus.json has been updated with page counts, diagnostic summaries, and page selections
6. page_selection.json contains the machine-readable selection
</verification>

<success_criteria>
- Baseline data exists for all 4 corpus documents with diagnostic sidecars and OCR text
- Coverage matrix reveals which struggle categories are represented across the corpus
- Page selection is coverage-driven (not count-driven) with both difficult and regression sets
- Selected pages are rendered as PNGs ready for Opus transcription in Plan 03
- Manifest reflects actual baseline data, not placeholder values
</success_criteria>

<output>
After completion, create `.planning/phases/16-test-corpus-ground-truth/16-02-SUMMARY.md`
</output>
