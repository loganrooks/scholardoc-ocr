---
phase: 13-model-caching
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/scholardoc_ocr/model_cache.py
  - pyproject.toml
  - tests/test_model_cache.py
autonomous: true

must_haves:
  truths:
    - "ModelCache returns cached models on second call without reloading"
    - "Cache expires after configured TTL (default 30 minutes)"
    - "Thread-safe access prevents race conditions on concurrent requests"
    - "GPU memory is cleared when cache evicted"
  artifacts:
    - path: "src/scholardoc_ocr/model_cache.py"
      provides: "ModelCache singleton with TTL-based caching"
      exports: ["ModelCache", "cleanup_between_documents", "get_memory_stats"]
      min_lines: 80
    - path: "tests/test_model_cache.py"
      provides: "Unit tests for cache behavior"
      min_lines: 50
  key_links:
    - from: "src/scholardoc_ocr/model_cache.py"
      to: "surya.load_models()"
      via: "lazy import in get_models()"
      pattern: "from . import surya"
    - from: "src/scholardoc_ocr/model_cache.py"
      to: "cachetools.TTLCache"
      via: "import and instantiation"
      pattern: "TTLCache"
---

<objective>
Create the ModelCache module with thread-safe singleton caching, TTL-based expiration, and GPU memory cleanup utilities.

Purpose: Provide the core caching infrastructure that eliminates 30-60 second model loading delays between MCP requests. This is the foundation for MODEL-01, MODEL-02, and MODEL-03 requirements.

Output: A new `model_cache.py` module with `ModelCache` class, `cleanup_between_documents()` and `get_memory_stats()` utilities, plus comprehensive unit tests.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/13-model-caching/13-RESEARCH.md

@src/scholardoc_ocr/surya.py
@pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add cachetools dependency</name>
  <files>pyproject.toml</files>
  <action>
Add cachetools>=5.0.0 to the dependencies list in pyproject.toml.

Place it alphabetically among existing dependencies:
- After "marker-pdf>=1.0.0"
- Before "ocrmypdf>=16.0.0"

No need to run pip install - the executor will handle that if needed.
  </action>
  <verify>grep -q "cachetools" pyproject.toml</verify>
  <done>cachetools>=5.0.0 appears in pyproject.toml dependencies</done>
</task>

<task type="auto">
  <name>Task 2: Create model_cache.py module</name>
  <files>src/scholardoc_ocr/model_cache.py</files>
  <action>
Create a new module with:

1. **ModelCache class** (thread-safe singleton with TTL):
   - Class-level `_instance` and `_lock` for double-checked locking singleton
   - Instance `_cache_lock` for thread-safe cache operations
   - `_cache: TTLCache[str, tuple[dict[str, Any], str]]` with maxsize=1
   - `_load_time: float | None` for tracking when models were loaded

   Methods:
   - `get_instance(cls, ttl_seconds: float = 1800.0)` - singleton accessor, default 30 min TTL
   - `get_models(self, device: str | None = None)` - returns cached models or loads fresh
     - Check cache under lock, return if hit
     - Load outside lock to avoid blocking (from . import surya; surya.load_models(device))
     - Store result under lock with timestamp
   - `is_loaded(self)` - check if models currently cached
   - `evict(self)` - force eviction and GPU cleanup
   - `_cleanup_gpu_memory(self)` - internal helper for torch.mps.empty_cache() / torch.cuda.empty_cache()

2. **cleanup_between_documents()** function:
   - Standalone function (not method) for use in pipeline
   - Calls torch.mps.empty_cache() if MPS available
   - Calls torch.cuda.empty_cache() if CUDA available
   - Calls gc.collect()
   - Does NOT evict the model cache (only clears unused GPU memory)

3. **get_memory_stats()** function:
   - Returns dict with device, allocated_mb, reserved_mb
   - Uses torch.cuda.memory_allocated() for CUDA
   - Uses torch.mps.current_allocated_memory() for MPS
   - Returns zeros with device="unknown" if torch not available

All torch imports must be lazy (inside functions/methods) to avoid loading ML deps at module import time.

Environment variable support:
- Read `SCHOLARDOC_MODEL_TTL` in get_instance() to allow runtime override of TTL

Use type annotations throughout. Add docstrings to all public functions/methods.
  </action>
  <verify>python -c "from scholardoc_ocr.model_cache import ModelCache, cleanup_between_documents, get_memory_stats; print('imports OK')"</verify>
  <done>model_cache.py exists with ModelCache, cleanup_between_documents, and get_memory_stats</done>
</task>

<task type="auto">
  <name>Task 3: Create unit tests for model_cache</name>
  <files>tests/test_model_cache.py</files>
  <action>
Create comprehensive unit tests for the model_cache module:

1. **Test singleton behavior:**
   - `test_get_instance_returns_same_instance` - verify singleton
   - `test_get_instance_respects_ttl_parameter` - first call sets TTL

2. **Test cache behavior (mock surya.load_models):**
   - `test_get_models_caches_result` - second call returns cached, load_models called once
   - `test_get_models_respects_device_parameter` - device passed through to load_models

3. **Test TTL expiration (mock time or use short TTL):**
   - `test_cache_expires_after_ttl` - use TTL=0.1s, sleep, verify reload

4. **Test eviction:**
   - `test_evict_clears_cache` - after evict, is_loaded() returns False
   - `test_evict_calls_gpu_cleanup` - mock torch, verify empty_cache called

5. **Test thread safety:**
   - `test_concurrent_get_models_loads_once` - use ThreadPoolExecutor with multiple threads
   - All threads should get same result, load_models called only once

6. **Test utility functions:**
   - `test_cleanup_between_documents_calls_gc` - mock gc.collect, verify called
   - `test_get_memory_stats_returns_dict` - verify keys exist

Use pytest fixtures for mocking. Use `@pytest.fixture(autouse=True)` to reset singleton between tests:

```python
@pytest.fixture(autouse=True)
def reset_singleton():
    """Reset ModelCache singleton between tests."""
    ModelCache._instance = None
    yield
    ModelCache._instance = None
```

Mock surya.load_models to avoid actually loading models in unit tests.
  </action>
  <verify>pytest tests/test_model_cache.py -v --tb=short</verify>
  <done>All unit tests pass</done>
</task>

</tasks>

<verification>
1. `grep -q "cachetools" pyproject.toml` - dependency added
2. `python -c "from scholardoc_ocr.model_cache import ModelCache"` - module imports
3. `pytest tests/test_model_cache.py -v` - all tests pass
4. `ruff check src/scholardoc_ocr/model_cache.py` - no lint errors
</verification>

<success_criteria>
- ModelCache class exists with get_instance(), get_models(), is_loaded(), evict() methods
- cachetools.TTLCache used for storage with configurable TTL (default 1800s)
- cleanup_between_documents() clears GPU memory without evicting cache
- get_memory_stats() returns GPU memory info dict
- All lazy torch imports (no import-time loading)
- All unit tests pass
- No ruff lint errors
</success_criteria>

<output>
After completion, create `.planning/phases/13-model-caching/13-01-SUMMARY.md`
</output>
