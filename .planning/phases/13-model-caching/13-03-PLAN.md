---
phase: 13-model-caching
plan: 03
type: execute
wave: 2
depends_on: ["13-01"]
files_modified:
  - src/scholardoc_ocr/mcp_server.py
  - tests/test_mcp_server.py
autonomous: true

must_haves:
  truths:
    - "MCP server can pre-load models at startup when SCHOLARDOC_WARM_LOAD=true"
    - "MCP server cleans up models on shutdown"
    - "ocr_memory_stats tool returns GPU memory usage and cache status"
  artifacts:
    - path: "src/scholardoc_ocr/mcp_server.py"
      provides: "MCP server with lifespan hooks and memory stats tool"
      contains: "ocr_memory_stats"
  key_links:
    - from: "src/scholardoc_ocr/mcp_server.py"
      to: "src/scholardoc_ocr/model_cache.py"
      via: "import in lifespan and tool"
      pattern: "from .model_cache import"
    - from: "mcp_lifespan"
      to: "ModelCache.get_models()"
      via: "warm loading on startup"
      pattern: "cache\\.get_models\\(\\)"
---

<objective>
Add MCP server lifespan hooks for warm-loading and shutdown cleanup, plus a memory stats tool for monitoring.

Purpose: Complete MODEL-04 (warm pool) and MODEL-05 (memory profiling) by integrating the caching infrastructure with the MCP server's lifecycle.

Output: Modified mcp_server.py with lifespan context manager, warm-loading on startup (configurable), cleanup on shutdown, and ocr_memory_stats tool.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/13-model-caching/13-01-SUMMARY.md

@src/scholardoc_ocr/mcp_server.py
@src/scholardoc_ocr/model_cache.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add lifespan hook to MCP server</name>
  <files>src/scholardoc_ocr/mcp_server.py</files>
  <action>
Add a lifespan context manager for warm-loading and shutdown cleanup:

1. **Add imports** near the top (after existing imports, before logger):
   ```python
   import os
   from contextlib import asynccontextmanager
   ```

2. **Add lifespan function** before the `mcp = FastMCP(...)` line:
   ```python
   @asynccontextmanager
   async def mcp_lifespan(server: FastMCP):
       """Lifespan hook for model warm-loading and cleanup.

       Environment variables:
           SCHOLARDOC_WARM_LOAD: Set to "true" to pre-load models at startup.
           SCHOLARDOC_MODEL_TTL: Model cache TTL in seconds (default 1800).
       """
       from .model_cache import ModelCache

       warm_load = os.environ.get("SCHOLARDOC_WARM_LOAD", "false").lower() == "true"
       ttl = float(os.environ.get("SCHOLARDOC_MODEL_TTL", "1800"))

       if warm_load:
           _log("Warm-loading models (SCHOLARDOC_WARM_LOAD=true)")
           cache = ModelCache.get_instance(ttl_seconds=ttl)
           cache.get_models()  # Triggers load
           _log("Models warm-loaded successfully")
       else:
           _log("Skipping warm-load (SCHOLARDOC_WARM_LOAD not set)")

       yield  # Server runs

       # Cleanup on shutdown
       _log("MCP server shutting down, evicting model cache")
       cache = ModelCache.get_instance()
       cache.evict()
       _log("Model cache evicted")
   ```

3. **Update FastMCP initialization** to use lifespan:

   BEFORE:
   ```python
   mcp = FastMCP("scholardoc-ocr")
   ```

   AFTER:
   ```python
   mcp = FastMCP("scholardoc-ocr", lifespan=mcp_lifespan)
   ```
  </action>
  <verify>grep -q "mcp_lifespan" src/scholardoc_ocr/mcp_server.py && grep -q "lifespan=mcp_lifespan" src/scholardoc_ocr/mcp_server.py</verify>
  <done>MCP server has lifespan hook with warm-loading and cleanup</done>
</task>

<task type="auto">
  <name>Task 2: Add ocr_memory_stats MCP tool</name>
  <files>src/scholardoc_ocr/mcp_server.py</files>
  <action>
Add a new MCP tool for monitoring GPU memory and cache status:

Add this after the existing tools (after ocr_status, before main()):

```python
@mcp.tool()
async def ocr_memory_stats() -> dict:
    """Get current GPU memory usage and model cache status.

    Returns memory statistics for debugging and monitoring. Useful for
    understanding resource utilization and diagnosing memory issues.

    Returns:
        dict with:
        - models_loaded: bool - whether models are currently cached
        - device: str - "mps", "cuda", or "unknown"
        - allocated_mb: float - GPU memory currently allocated
        - reserved_mb: float - GPU memory reserved (CUDA only, 0 for MPS)
        - cache_ttl_seconds: float - configured cache TTL
    """
    from .model_cache import ModelCache, get_memory_stats

    cache = ModelCache.get_instance()
    memory = get_memory_stats()

    return {
        "models_loaded": cache.is_loaded(),
        "device": memory.get("device", "unknown"),
        "allocated_mb": round(memory.get("allocated_mb", 0), 2),
        "reserved_mb": round(memory.get("reserved_mb", 0), 2),
        "cache_ttl_seconds": cache._ttl,
    }
```

Note: Accessing `cache._ttl` is acceptable here for debugging purposes. It's a private attribute but we're in the same package.
  </action>
  <verify>grep -q "ocr_memory_stats" src/scholardoc_ocr/mcp_server.py</verify>
  <done>ocr_memory_stats tool added to MCP server</done>
</task>

<task type="auto">
  <name>Task 3: Create MCP server tests</name>
  <files>tests/test_mcp_server.py</files>
  <action>
Create a new test file for MCP server functionality:

```python
"""Tests for MCP server model caching integration."""

from __future__ import annotations

import pytest


class TestMCPLifespan:
    """Tests for MCP server lifespan hooks."""

    def test_lifespan_warm_load_env_var(self, mocker, monkeypatch):
        """Test that SCHOLARDOC_WARM_LOAD=true triggers model loading."""
        monkeypatch.setenv("SCHOLARDOC_WARM_LOAD", "true")

        # Mock ModelCache
        mock_cache = mocker.MagicMock()
        mock_get_instance = mocker.patch(
            "scholardoc_ocr.mcp_server.ModelCache.get_instance",
            return_value=mock_cache
        )

        # Import after env var set
        from scholardoc_ocr.mcp_server import mcp_lifespan

        # Run lifespan
        import asyncio

        async def run_lifespan():
            async with mcp_lifespan(None):
                pass

        asyncio.run(run_lifespan())

        # Verify warm load happened
        mock_cache.get_models.assert_called_once()
        mock_cache.evict.assert_called_once()  # Cleanup on exit

    def test_lifespan_no_warm_load_by_default(self, mocker, monkeypatch):
        """Test that warm load is skipped by default."""
        monkeypatch.delenv("SCHOLARDOC_WARM_LOAD", raising=False)

        mock_cache = mocker.MagicMock()
        mock_get_instance = mocker.patch(
            "scholardoc_ocr.mcp_server.ModelCache.get_instance",
            return_value=mock_cache
        )

        from scholardoc_ocr.mcp_server import mcp_lifespan

        import asyncio

        async def run_lifespan():
            async with mcp_lifespan(None):
                pass

        asyncio.run(run_lifespan())

        # get_models NOT called during startup (only evict on cleanup)
        # The get_instance call happens but get_models should not be called
        # Actually, get_instance is only called in cleanup path if not warm_load
        mock_cache.evict.assert_called_once()


class TestOCRMemoryStats:
    """Tests for ocr_memory_stats tool."""

    @pytest.mark.asyncio
    async def test_ocr_memory_stats_returns_expected_keys(self, mocker):
        """Test that ocr_memory_stats returns correct structure."""
        # Mock ModelCache
        mock_cache = mocker.MagicMock()
        mock_cache.is_loaded.return_value = False
        mock_cache._ttl = 1800.0
        mocker.patch(
            "scholardoc_ocr.mcp_server.ModelCache.get_instance",
            return_value=mock_cache
        )

        # Mock get_memory_stats
        mocker.patch(
            "scholardoc_ocr.mcp_server.get_memory_stats",
            return_value={"device": "cpu", "allocated_mb": 0, "reserved_mb": 0}
        )

        from scholardoc_ocr.mcp_server import ocr_memory_stats

        result = await ocr_memory_stats()

        assert "models_loaded" in result
        assert "device" in result
        assert "allocated_mb" in result
        assert "reserved_mb" in result
        assert "cache_ttl_seconds" in result
        assert result["models_loaded"] is False
        assert result["cache_ttl_seconds"] == 1800.0

    @pytest.mark.asyncio
    async def test_ocr_memory_stats_with_loaded_models(self, mocker):
        """Test ocr_memory_stats when models are loaded."""
        mock_cache = mocker.MagicMock()
        mock_cache.is_loaded.return_value = True
        mock_cache._ttl = 3600.0
        mocker.patch(
            "scholardoc_ocr.mcp_server.ModelCache.get_instance",
            return_value=mock_cache
        )

        mocker.patch(
            "scholardoc_ocr.mcp_server.get_memory_stats",
            return_value={"device": "mps", "allocated_mb": 512.5, "reserved_mb": 0}
        )

        from scholardoc_ocr.mcp_server import ocr_memory_stats

        result = await ocr_memory_stats()

        assert result["models_loaded"] is True
        assert result["device"] == "mps"
        assert result["allocated_mb"] == 512.5
```

Add pytest-asyncio to dev dependencies if not present (it likely already is via other deps).
  </action>
  <verify>pytest tests/test_mcp_server.py -v --tb=short</verify>
  <done>MCP server tests pass</done>
</task>

</tasks>

<verification>
1. `grep -q "mcp_lifespan" src/scholardoc_ocr/mcp_server.py` - lifespan function exists
2. `grep -q "ocr_memory_stats" src/scholardoc_ocr/mcp_server.py` - tool exists
3. `pytest tests/test_mcp_server.py -v` - all tests pass
4. `ruff check src/scholardoc_ocr/mcp_server.py` - no lint errors
</verification>

<success_criteria>
- mcp_lifespan context manager handles startup warm-loading and shutdown cleanup
- SCHOLARDOC_WARM_LOAD=true triggers model pre-loading
- SCHOLARDOC_MODEL_TTL allows runtime TTL configuration
- ocr_memory_stats tool returns models_loaded, device, allocated_mb, reserved_mb, cache_ttl_seconds
- All tests pass
- No ruff lint errors
</success_criteria>

<output>
After completion, create `.planning/phases/13-model-caching/13-03-SUMMARY.md`
</output>
