---
phase: 11-benchmarking-foundation-metrics-fixes
plan: 02
type: execute
wave: 2
depends_on: ["11-01"]
files_modified:
  - tests/benchmarks/test_model_loading.py
  - tests/benchmarks/test_inference.py
  - tests/benchmarks/test_memory.py
autonomous: true

must_haves:
  truths:
    - "pytest --benchmark-only runs model loading benchmark with pedantic mode"
    - "pytest --benchmark-only runs inference benchmark with MPS synchronization"
    - "pytest --memray runs memory limit tests"
  artifacts:
    - path: "tests/benchmarks/test_model_loading.py"
      provides: "Cold-start model loading benchmark"
      contains: "benchmark.pedantic"
    - path: "tests/benchmarks/test_inference.py"
      provides: "OCR inference benchmark with GPU sync"
      contains: "mps_sync"
    - path: "tests/benchmarks/test_memory.py"
      provides: "Memory limit and leak tests"
      contains: "limit_memory"
  key_links:
    - from: "tests/benchmarks/test_model_loading.py"
      to: "src/scholardoc_ocr/surya.py"
      via: "surya.load_models()"
      pattern: "surya\\.load_models"
    - from: "tests/benchmarks/test_inference.py"
      to: "src/scholardoc_ocr/timing.py"
      via: "MPS synchronization"
      pattern: "mps_sync"
---

<objective>
Write benchmark tests for model loading, OCR inference, and memory limits.

Purpose: BENCH-01 (baseline measurements), BENCH-02 (correct timing with MPS sync), BENCH-04 (memory profiling) require actual benchmark tests.

Output: Three benchmark test files covering model loading, inference, and memory constraints.
</objective>

<execution_context>
@/Users/rookslog/.claude/get-shit-done/workflows/execute-plan.md
@/Users/rookslog/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/11-benchmarking-foundation-metrics-fixes/11-RESEARCH.md
@.planning/phases/11-benchmarking-foundation-metrics-fixes/11-01-SUMMARY.md

@src/scholardoc_ocr/surya.py
@src/scholardoc_ocr/timing.py
@tests/benchmarks/conftest.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create model loading benchmark</name>
  <files>tests/benchmarks/test_model_loading.py</files>
  <action>
Create tests/benchmarks/test_model_loading.py:

```python
"""Benchmark tests for Surya model loading time (BENCH-01, BENCH-02)."""
import pytest

from scholardoc_ocr import surya
from scholardoc_ocr.timing import mps_sync

pytestmark = pytest.mark.skipif(
    not surya.is_available(),
    reason="Marker/Surya not installed"
)


def test_model_loading_cold_start(benchmark, hardware_profile):
    """Benchmark Surya model loading (cold start).

    Measures the time to load all Surya/Marker models from disk.
    This is the baseline for model caching optimization (Phase 13).
    """
    def load_models():
        models = surya.load_models()
        mps_sync()  # Ensure GPU work completes before timing stops
        return models

    # Use pedantic mode for expensive operations
    # warmup_rounds=0 for true cold-start measurement
    # rounds=3 for statistical validity without excessive time
    result = benchmark.pedantic(
        load_models,
        rounds=3,
        warmup_rounds=0,
        iterations=1,
    )
    assert result is not None
    assert "layout" in result or "detection" in result  # Has expected model keys
```

Key points:
- Use pytest.mark.skipif to skip if Surya not available
- benchmark.pedantic() with rounds=3, warmup_rounds=0, iterations=1
- mps_sync() after model load to ensure GPU work completes
- Assert model dict has expected keys
  </action>
  <verify>
    pytest tests/benchmarks/test_model_loading.py --benchmark-only -v (runs or skips cleanly)
  </verify>
  <done>
    Model loading benchmark uses pedantic mode with proper MPS synchronization
  </done>
</task>

<task type="auto">
  <name>Task 2: Create inference and memory benchmarks</name>
  <files>
    tests/benchmarks/test_inference.py
    tests/benchmarks/test_memory.py
  </files>
  <action>
1. Create tests/benchmarks/test_inference.py:

```python
"""Benchmark tests for Surya OCR inference (BENCH-01, BENCH-02)."""
import pytest

from scholardoc_ocr import surya
from scholardoc_ocr.surya import SuryaConfig
from scholardoc_ocr.timing import mps_sync

pytestmark = pytest.mark.skipif(
    not surya.is_available(),
    reason="Marker/Surya not installed"
)


def test_single_page_inference(benchmark, loaded_models, sample_pdf):
    """Benchmark single-page Surya inference.

    Uses pre-loaded models (session fixture) to measure pure inference time.
    """
    def run_inference():
        result = surya.convert_pdf(
            sample_pdf,
            loaded_models,
            config=SuryaConfig(langs="en"),
            page_range=[0],  # Single page
        )
        mps_sync()
        return result

    result = benchmark.pedantic(
        run_inference,
        rounds=5,
        warmup_rounds=1,
        iterations=1,
    )
    assert isinstance(result, str)
    assert len(result) > 0
```

2. Create tests/benchmarks/test_memory.py:

```python
"""Memory limit tests for OCR processing (BENCH-04)."""
import pytest

from scholardoc_ocr import surya
from scholardoc_ocr.surya import SuryaConfig

pytestmark = pytest.mark.skipif(
    not surya.is_available(),
    reason="Marker/Surya not installed"
)


@pytest.mark.limit_memory("2 GB")
def test_model_loading_memory(loaded_models):
    """Model loading should not exceed 2GB peak memory.

    This validates memory usage is reasonable for Apple Silicon devices.
    The loaded_models fixture triggers the load; this test just checks
    the memory limit marker.
    """
    assert loaded_models is not None


@pytest.mark.limit_memory("4 GB")
def test_single_page_inference_memory(loaded_models, sample_pdf):
    """Single page OCR should not exceed 4GB peak memory."""
    result = surya.convert_pdf(
        sample_pdf,
        loaded_models,
        config=SuryaConfig(langs="en"),
        page_range=[0],
    )
    assert len(result) > 0
```

Key points:
- Inference test uses loaded_models fixture (session-scoped, already loaded)
- Memory tests use @pytest.mark.limit_memory decorator
- All tests skip gracefully if Surya not installed
  </action>
  <verify>
    pytest tests/benchmarks/test_inference.py --benchmark-only -v
    pytest tests/benchmarks/test_memory.py --memray -v (if memray available)
  </verify>
  <done>
    Inference benchmark measures per-page processing time with MPS sync, memory tests enforce limits
  </done>
</task>

</tasks>

<verification>
- pytest tests/benchmarks/ --benchmark-only --collect-only lists all benchmark tests
- pytest tests/benchmarks/test_model_loading.py --benchmark-only (runs or skips if no Surya)
- pytest tests/benchmarks/test_inference.py --benchmark-only (runs with loaded_models fixture)
- pytest tests/benchmarks/ --benchmark-json=output.json produces JSON output (for CI)
</verification>

<success_criteria>
- Three benchmark test files exist in tests/benchmarks/
- Tests use pedantic mode for expensive GPU operations
- Tests call mps_sync() before timing completes
- Tests skip gracefully when Surya not installed
- Memory tests use limit_memory markers
</success_criteria>

<output>
After completion, create `.planning/phases/11-benchmarking-foundation-metrics-fixes/11-02-SUMMARY.md`
</output>
