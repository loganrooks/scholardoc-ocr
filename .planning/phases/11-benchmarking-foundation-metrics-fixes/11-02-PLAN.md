---
phase: 11-benchmarking-foundation-metrics-fixes
plan: 02
type: execute
wave: 2
depends_on: ["11-01"]
files_modified:
  - tests/benchmarks/test_model_loading.py
  - tests/benchmarks/test_inference.py
  - tests/benchmarks/test_memory.py
  - pyproject.toml
autonomous: true

must_haves:
  truths:
    - "pytest --benchmark-only runs model loading benchmark with pedantic mode"
    - "pytest --benchmark-only runs inference benchmark with MPS synchronization"
    - "pytest --memray runs memory limit tests"
    - "Benchmark results are grouped by hardware profile for baseline selection (BENCH-05)"
  artifacts:
    - path: "tests/benchmarks/test_model_loading.py"
      provides: "Cold-start model loading benchmark"
      contains: "benchmark.pedantic"
    - path: "tests/benchmarks/test_inference.py"
      provides: "OCR inference benchmark with GPU sync"
      contains: "mps_sync"
    - path: "tests/benchmarks/test_memory.py"
      provides: "Memory limit and leak tests"
      contains: "limit_memory"
    - path: "pyproject.toml"
      provides: "pytest-benchmark group configuration"
      contains: "benchmark-group-by"
  key_links:
    - from: "tests/benchmarks/test_model_loading.py"
      to: "src/scholardoc_ocr/surya.py"
      via: "surya.load_models()"
      pattern: "surya\\.load_models"
    - from: "tests/benchmarks/test_inference.py"
      to: "src/scholardoc_ocr/timing.py"
      via: "MPS synchronization"
      pattern: "mps_sync"
    - from: "pyproject.toml"
      to: "tests/benchmarks/conftest.py"
      via: "hardware_profile fixture used in group-by"
      pattern: "benchmark-group-by.*param:hardware_profile"
---

<objective>
Write benchmark tests for model loading, OCR inference, and memory limits. Configure pytest-benchmark to group results by hardware profile for BENCH-05 baseline selection.

Purpose: BENCH-01 (baseline measurements), BENCH-02 (correct timing with MPS sync), BENCH-04 (memory profiling), BENCH-05 (hardware-specific profiles) require actual benchmark tests and proper grouping.

Output: Three benchmark test files covering model loading, inference, and memory constraints. Updated pyproject.toml with benchmark grouping config.
</objective>

<execution_context>
@/Users/rookslog/.claude/get-shit-done/workflows/execute-plan.md
@/Users/rookslog/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/11-benchmarking-foundation-metrics-fixes/11-RESEARCH.md
@.planning/phases/11-benchmarking-foundation-metrics-fixes/11-01-SUMMARY.md

@src/scholardoc_ocr/surya.py
@src/scholardoc_ocr/timing.py
@tests/benchmarks/conftest.py
@pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create model loading benchmark with hardware profile grouping</name>
  <files>
    tests/benchmarks/test_model_loading.py
    pyproject.toml
  </files>
  <action>
1. Create tests/benchmarks/test_model_loading.py:

```python
"""Benchmark tests for Surya model loading time (BENCH-01, BENCH-02, BENCH-05)."""
import pytest

from scholardoc_ocr import surya
from scholardoc_ocr.timing import mps_sync

pytestmark = pytest.mark.skipif(
    not surya.is_available(),
    reason="Marker/Surya not installed"
)


def test_model_loading_cold_start(benchmark, hardware_profile):
    """Benchmark Surya model loading (cold start).

    Measures the time to load all Surya/Marker models from disk.
    This is the baseline for model caching optimization (Phase 13).

    The hardware_profile fixture (M1/M2/M3/M4/cpu) is used by pytest-benchmark
    to group results, enabling hardware-specific baseline comparisons (BENCH-05).
    """
    def load_models():
        models = surya.load_models()
        mps_sync()  # Ensure GPU work completes before timing stops
        return models

    # Use pedantic mode for expensive operations
    # warmup_rounds=0 for true cold-start measurement
    # rounds=3 for statistical validity without excessive time
    result = benchmark.pedantic(
        load_models,
        rounds=3,
        warmup_rounds=0,
        iterations=1,
    )
    assert result is not None
    assert "layout" in result or "detection" in result  # Has expected model keys
```

2. Update pyproject.toml to add benchmark configuration under [tool.pytest.ini_options]:

```toml
[tool.pytest.ini_options]
# ... existing options ...
# Benchmark grouping by hardware profile for BENCH-05 baseline selection
# The hardware_profile fixture returns M1/M2/M3/M4/cpu
addopts = "--benchmark-group-by=param:hardware_profile,func"
benchmark_min_rounds = 3
```

This ensures benchmark results are grouped by both the hardware_profile parameter AND the function name, allowing github-action-benchmark to compare M1 baselines to M1 runs, M2 to M2, etc.

Key points:
- Use pytest.mark.skipif to skip if Surya not available
- benchmark.pedantic() with rounds=3, warmup_rounds=0, iterations=1
- mps_sync() after model load to ensure GPU work completes
- hardware_profile fixture enables BENCH-05 hardware-specific baselines
- pyproject.toml configures grouping so CI compares like-to-like hardware
  </action>
  <verify>
    grep "benchmark-group-by" pyproject.toml
    pytest tests/benchmarks/test_model_loading.py --benchmark-only -v (runs or skips cleanly)
  </verify>
  <done>
    Model loading benchmark uses pedantic mode with proper MPS synchronization and hardware profile grouping
  </done>
</task>

<task type="auto">
  <name>Task 2: Create inference and memory benchmarks</name>
  <files>
    tests/benchmarks/test_inference.py
    tests/benchmarks/test_memory.py
  </files>
  <action>
1. Create tests/benchmarks/test_inference.py:

```python
"""Benchmark tests for Surya OCR inference (BENCH-01, BENCH-02, BENCH-05)."""
import pytest

from scholardoc_ocr import surya
from scholardoc_ocr.surya import SuryaConfig
from scholardoc_ocr.timing import mps_sync

pytestmark = pytest.mark.skipif(
    not surya.is_available(),
    reason="Marker/Surya not installed"
)


def test_single_page_inference(benchmark, loaded_models, sample_pdf, hardware_profile):
    """Benchmark single-page Surya inference.

    Uses pre-loaded models (session fixture) to measure pure inference time.
    The hardware_profile fixture enables BENCH-05 hardware-specific baselines.
    """
    def run_inference():
        result = surya.convert_pdf(
            sample_pdf,
            loaded_models,
            config=SuryaConfig(langs="en"),
            page_range=[0],  # Single page
        )
        mps_sync()
        return result

    result = benchmark.pedantic(
        run_inference,
        rounds=5,
        warmup_rounds=1,
        iterations=1,
    )
    assert isinstance(result, str)
    assert len(result) > 0
```

2. Create tests/benchmarks/test_memory.py:

```python
"""Memory limit tests for OCR processing (BENCH-04)."""
import pytest

from scholardoc_ocr import surya
from scholardoc_ocr.surya import SuryaConfig

pytestmark = pytest.mark.skipif(
    not surya.is_available(),
    reason="Marker/Surya not installed"
)


@pytest.mark.limit_memory("2 GB")
def test_model_loading_memory(loaded_models):
    """Model loading should not exceed 2GB peak memory.

    This validates memory usage is reasonable for Apple Silicon devices.
    The loaded_models fixture triggers the load; this test just checks
    the memory limit marker.
    """
    assert loaded_models is not None


@pytest.mark.limit_memory("4 GB")
def test_single_page_inference_memory(loaded_models, sample_pdf):
    """Single page OCR should not exceed 4GB peak memory."""
    result = surya.convert_pdf(
        sample_pdf,
        loaded_models,
        config=SuryaConfig(langs="en"),
        page_range=[0],
    )
    assert len(result) > 0
```

Key points:
- Inference test uses loaded_models fixture (session-scoped, already loaded)
- Inference test includes hardware_profile for BENCH-05 grouping
- Memory tests use @pytest.mark.limit_memory decorator
- All tests skip gracefully if Surya not installed
  </action>
  <verify>
    pytest tests/benchmarks/test_inference.py --benchmark-only -v
    pytest tests/benchmarks/test_memory.py --memray -v (if memray available)
  </verify>
  <done>
    Inference benchmark measures per-page processing time with MPS sync and hardware grouping, memory tests enforce limits
  </done>
</task>

</tasks>

<verification>
- pytest tests/benchmarks/ --benchmark-only --collect-only lists all benchmark tests
- pytest tests/benchmarks/test_model_loading.py --benchmark-only (runs or skips if no Surya)
- pytest tests/benchmarks/test_inference.py --benchmark-only (runs with loaded_models fixture)
- pytest tests/benchmarks/ --benchmark-json=output.json produces JSON output with hardware grouping
- grep "benchmark-group-by" pyproject.toml shows param:hardware_profile config
</verification>

<success_criteria>
- Three benchmark test files exist in tests/benchmarks/
- Tests use pedantic mode for expensive GPU operations
- Tests call mps_sync() before timing completes
- Tests skip gracefully when Surya not installed
- Memory tests use limit_memory markers
- pyproject.toml configures benchmark grouping by hardware_profile (BENCH-05)
- Benchmark tests include hardware_profile fixture for baseline selection
</success_criteria>

<output>
After completion, create `.planning/phases/11-benchmarking-foundation-metrics-fixes/11-02-SUMMARY.md`
</output>
