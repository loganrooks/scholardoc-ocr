---
phase: 11-benchmarking-foundation-metrics-fixes
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - tests/benchmarks/__init__.py
  - tests/benchmarks/conftest.py
  - src/scholardoc_ocr/timing.py
autonomous: true

must_haves:
  truths:
    - "pytest-benchmark and pytest-memray are installable via pip install -e .[dev]"
    - "Hardware profile detection returns M1/M2/M3/M4 or cpu"
    - "GPU-aware timing utilities synchronize MPS before measuring"
  artifacts:
    - path: "pyproject.toml"
      provides: "Benchmark dev dependencies"
      contains: "pytest-benchmark"
    - path: "tests/benchmarks/conftest.py"
      provides: "Benchmark fixtures (hardware_profile, loaded_models)"
      min_lines: 50
    - path: "src/scholardoc_ocr/timing.py"
      provides: "MPS-aware timing context manager"
      exports: ["mps_timed", "get_hardware_profile"]
  key_links:
    - from: "tests/benchmarks/conftest.py"
      to: "src/scholardoc_ocr/timing.py"
      via: "import for hardware detection"
      pattern: "from scholardoc_ocr.timing import"
---

<objective>
Set up benchmarking infrastructure: dependencies, fixtures, and GPU-aware timing utilities.

Purpose: BENCH-01 (infrastructure), BENCH-02 (timing methodology), BENCH-04 (memray), BENCH-05 (hardware profiles) require foundational tooling before benchmark tests can be written.

Output: Updated pyproject.toml with benchmark deps, tests/benchmarks/ directory with conftest.py, timing utility module.
</objective>

<execution_context>
@/Users/rookslog/.claude/get-shit-done/workflows/execute-plan.md
@/Users/rookslog/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/11-benchmarking-foundation-metrics-fixes/11-RESEARCH.md

@src/scholardoc_ocr/surya.py
@pyproject.toml
@tests/conftest.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add benchmark dependencies and timing module</name>
  <files>
    pyproject.toml
    src/scholardoc_ocr/timing.py
  </files>
  <action>
1. Update pyproject.toml [project.optional-dependencies].dev to add:
   - "pytest-benchmark>=5.0"
   - "pytest-memray>=1.0"
   - "memray>=1.0"

2. Create src/scholardoc_ocr/timing.py with:
   - `get_hardware_profile() -> str`: Detect Apple Silicon variant (M1/M2/M3/M4) via `sysctl -n machdep.cpu.brand_string` on darwin, return "cpu" on other platforms or non-ARM
   - `mps_available() -> bool`: Check torch.backends.mps.is_available() with lazy torch import
   - `mps_sync()`: Call torch.mps.synchronize() if MPS available, no-op otherwise
   - `@contextmanager mps_timed(name: str) -> Generator[dict, None, None]`: Context manager that yields a dict, measures wall-clock time with MPS sync before stop, populates dict with {"elapsed": float}

Use lazy torch imports (inside function bodies) to avoid loading torch at module import time.
  </action>
  <verify>
    pip install -e ".[dev]" succeeds
    python -c "from scholardoc_ocr.timing import get_hardware_profile, mps_timed; print(get_hardware_profile())"
  </verify>
  <done>
    Benchmark dependencies installable, timing module provides hardware detection and MPS-aware timing
  </done>
</task>

<task type="auto">
  <name>Task 2: Create benchmark test fixtures</name>
  <files>
    tests/benchmarks/__init__.py
    tests/benchmarks/conftest.py
  </files>
  <action>
1. Create tests/benchmarks/__init__.py (empty file)

2. Create tests/benchmarks/conftest.py with:
   - `@pytest.fixture(scope="session") def hardware_profile() -> str`: Returns get_hardware_profile()
   - `@pytest.fixture(scope="session") def loaded_models()`: Lazily loads Surya models once per session using surya.load_models(), includes MPS sync after load. Returns model dict. Skips if marker not installed.
   - `@pytest.fixture def sample_pdf(tmp_path) -> Path`: Creates a minimal 1-page PDF with text for benchmarking. Use pymupdf to create programmatically (not external fixture file).
   - Register memray markers in pytest_configure: limit_memory, limit_leaks

Import timing utilities from scholardoc_ocr.timing.
  </action>
  <verify>
    pytest tests/benchmarks/ --collect-only shows fixtures available
    pytest tests/benchmarks/conftest.py -v (runs fixture collection without errors)
  </verify>
  <done>
    Benchmark fixtures provide hardware_profile, loaded_models (session-scoped), sample_pdf, and memray markers are registered
  </done>
</task>

</tasks>

<verification>
- pip install -e ".[dev]" installs pytest-benchmark, pytest-memray, memray
- python -c "from scholardoc_ocr.timing import get_hardware_profile; print(get_hardware_profile())" outputs M1/M2/M3/M4 or cpu
- pytest tests/benchmarks/ --collect-only lists fixtures without import errors
</verification>

<success_criteria>
- Benchmark dependencies added to pyproject.toml
- timing.py exists with get_hardware_profile(), mps_timed(), mps_sync()
- tests/benchmarks/conftest.py exists with session fixtures
- No import errors when collecting benchmark tests
</success_criteria>

<output>
After completion, create `.planning/phases/11-benchmarking-foundation-metrics-fixes/11-01-SUMMARY.md`
</output>
