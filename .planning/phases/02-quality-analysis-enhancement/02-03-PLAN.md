---
phase: 02-quality-analysis-enhancement
plan: 03
type: execute
wave: 2
depends_on: ["02-01", "02-02"]
files_modified:
  - src/scholardoc_ocr/quality.py
  - tests/test_quality.py
autonomous: true

must_haves:
  truths:
    - "Composite quality score combines garbled regex, dictionary, and confidence signals"
    - "Per-page quality breakdown with individual signal scores available in results"
    - "Gray zone pages near threshold trigger additional analysis"
    - "Missing signals (e.g., no confidence data) handled gracefully with reweighting"
    - "Quality analysis has comprehensive unit tests"
    - "German language support configurable (Tesseract: deu, Surya: de)"
  artifacts:
    - path: "src/scholardoc_ocr/quality.py"
      provides: "CompositeQualityAnalyzer with multi-signal scoring"
      exports: ["QualityAnalyzer", "QualityResult", "SignalResult"]
      contains: "CompositeQualityAnalyzer"
    - path: "tests/test_quality.py"
      provides: "Unit tests for composite scoring, signals, edge cases"
      min_lines: 100
  key_links:
    - from: "src/scholardoc_ocr/quality.py"
      to: "src/scholardoc_ocr/dictionary.py"
      via: "DictionarySignal import and usage"
      pattern: "from.*dictionary.*import.*DictionarySignal"
    - from: "src/scholardoc_ocr/quality.py"
      to: "src/scholardoc_ocr/confidence.py"
      via: "ConfidenceSignal import and usage"
      pattern: "from.*confidence.*import.*ConfidenceSignal"
    - from: "src/scholardoc_ocr/quality.py"
      to: "src/scholardoc_ocr/types.py"
      via: "QualityResult used by pipeline"
      pattern: "QualityResult"
---

<objective>
Refactor QualityAnalyzer into a composite multi-signal scorer and add comprehensive tests.

Purpose: Combines all three quality signals (garbled regex, dictionary, confidence) into a single composite scorer with per-signal breakdown, gray zone handling, and configurable thresholds. This is the integration point for the phase.
Output: Refactored quality.py with CompositeQualityAnalyzer, comprehensive test_quality.py.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-quality-analysis-enhancement/02-RESEARCH.md
@.planning/phases/02-quality-analysis-enhancement/02-01-SUMMARY.md
@.planning/phases/02-quality-analysis-enhancement/02-02-SUMMARY.md
@src/scholardoc_ocr/quality.py
@src/scholardoc_ocr/types.py
@src/scholardoc_ocr/dictionary.py
@src/scholardoc_ocr/confidence.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Refactor QualityAnalyzer into composite multi-signal scorer</name>
  <files>src/scholardoc_ocr/quality.py</files>
  <action>
Refactor quality.py to integrate all three signals into a composite scorer. Keep backward compatibility.

1. Import SignalResult from dictionary.py. Import DictionarySignal from dictionary.py. Import ConfidenceSignal from confidence.py.

2. Extend QualityResult dataclass with new fields (backward compatible — new fields have defaults):
   ```python
   @dataclass
   class QualityResult:
       score: float
       flagged: bool
       garbled_count: int
       total_words: int
       sample_issues: list[str] = field(default_factory=list)
       sample_context: list[str] = field(default_factory=list)
       # NEW composite fields
       signal_scores: dict[str, float] = field(default_factory=dict)
       signal_details: dict[str, dict] = field(default_factory=dict)
       confidence_mean: float | None = None
       snippets: list[str] = field(default_factory=list)  # Problematic text samples
   ```

3. Rename existing QualityAnalyzer to _GarbledSignal (internal) that returns a SignalResult. Keep all existing regex logic intact.

4. Create `QualityAnalyzer` (replaces old one, same name for backward compat) as the composite:
   ```python
   class QualityAnalyzer:
       GRAY_ZONE = 0.05  # threshold +/- this

       def __init__(
           self,
           threshold: float = 0.85,
           max_samples: int = 10,
           signal_floors: dict[str, float] | None = None,
           languages: list[str] | None = None,  # e.g. ["en", "de", "fr"]
           custom_vocab_path: Path | None = None,
       ):
           self.threshold = threshold
           self.max_samples = max_samples
           self.signal_floors = signal_floors or {"confidence": 0.3, "garbled": 0.5, "dictionary": 0.4}
           self.languages = languages or ["en", "fr"]
           self._garbled = _GarbledSignal(threshold=threshold, max_samples=max_samples)
           self._dictionary = DictionarySignal(custom_vocab_path=custom_vocab_path)
           self._confidence = ConfidenceSignal(langs=self._tesseract_langs())

       def _tesseract_langs(self) -> str:
           # Map ["en", "de", "fr"] -> "eng+deu+fra"
           lang_map = {"en": "eng", "de": "deu", "fr": "fra", "el": "ell", "la": "lat"}
           return "+".join(lang_map.get(l, l) for l in self.languages)

       def analyze(self, text: str, confidence_data: list[dict] | None = None, collect_context: bool = False) -> QualityResult:
           # Run garbled signal (always)
           garbled_result = self._garbled.score(text, collect_context)
           signals = {"garbled": garbled_result}

           # Run dictionary signal (always)
           dict_result = self._dictionary.score(text)
           signals["dictionary"] = dict_result

           # Run confidence signal (if data provided)
           if confidence_data is not None:
               conf_result = self._confidence.score_from_data(confidence_data)
               signals["confidence"] = conf_result

           # Combine signals
           composite_score = self._combine(signals)

           # Check per-signal floors
           floor_fail = any(
               s.score < self.signal_floors.get(s.name, 0)
               for s in signals.values()
           )

           # Short-circuit: very high or very low confidence
           if "confidence" in signals:
               conf_score = signals["confidence"].score
               if conf_score > 0.95:  # Very high confidence, trust it
                   composite_score = max(composite_score, 0.9)
               elif conf_score < 0.2:  # Very low confidence, flag it
                   composite_score = min(composite_score, 0.3)

           flagged = composite_score < self.threshold or floor_fail

           # Gray zone: if score is near threshold, note it
           in_gray_zone = abs(composite_score - self.threshold) < self.GRAY_ZONE

           return QualityResult(
               score=composite_score,
               flagged=flagged,
               garbled_count=garbled_result.details.get("garbled_count", 0),
               total_words=garbled_result.details.get("total_words", 0),
               sample_issues=garbled_result.details.get("sample_issues", []),
               sample_context=garbled_result.details.get("sample_context", []),
               signal_scores={name: s.score for name, s in signals.items()},
               signal_details={name: s.details for name, s in signals.items()},
               confidence_mean=signals.get("confidence", SignalResult("", 0, False, {})).details.get("mean_conf"),
               snippets=garbled_result.details.get("sample_issues", []),
           )

       def _combine(self, signals: dict[str, SignalResult]) -> float:
           # Weighted average. Weights depend on which signals are present.
           weights = {"garbled": 0.4, "dictionary": 0.3, "confidence": 0.3}
           if "confidence" not in signals:
               # Reweight without confidence
               weights = {"garbled": 0.55, "dictionary": 0.45}
           total_weight = sum(weights.get(name, 0) for name in signals)
           if total_weight == 0:
               return 0.5
           return sum(signals[name].score * weights.get(name, 0) for name in signals if name in weights) / total_weight

       def analyze_pages(self, page_texts: list[str], confidence_data_per_page: list[list[dict] | None] | None = None, collect_context: bool = False) -> list[QualityResult]:
           # Analyze each page with optional per-page confidence data
           if confidence_data_per_page is None:
               confidence_data_per_page = [None] * len(page_texts)
           return [
               self.analyze(text, conf_data, collect_context)
               for text, conf_data in zip(page_texts, confidence_data_per_page)
           ]

       def get_bad_pages(self, page_texts: list[str]) -> list[int]:
           # Backward compatible
           results = self.analyze_pages(page_texts)
           return [i for i, r in enumerate(results) if r.flagged]
   ```

The _GarbledSignal internal class keeps all existing regex logic from the current QualityAnalyzer but returns a SignalResult instead of QualityResult. It should have a `score(text, collect_context) -> SignalResult` method that runs the existing analysis and maps the result.

German language handling: the _GarbledSignal (which has VALID_TERMS, GERMAN_PHILOSOPHY_TERMS from plan 02-01) already handles German. The language config in the composite only affects Tesseract lang strings and potential future per-language tuning.
  </action>
  <verify>
`python -c "from scholardoc_ocr.quality import QualityAnalyzer, QualityResult; q = QualityAnalyzer(); r = q.analyze('The quick brown fox jumps over the lazy dog'); print(r.score, r.signal_scores)"` returns high score with per-signal breakdown.
`python -c "from scholardoc_ocr.quality import QualityAnalyzer; q = QualityAnalyzer(); r = q.analyze('xkjhf bvnmq zzzttt qwrtp'); print(r.score, r.flagged, r.signal_scores)"` returns low score, flagged=True.
`ruff check src/scholardoc_ocr/quality.py` passes.
  </verify>
  <done>QualityAnalyzer is a composite scorer combining garbled regex, dictionary, and optional confidence signals. Per-signal breakdown in QualityResult. Backward compatible API (analyze, analyze_pages, get_bad_pages). Gray zone detection. Signal floor checking. Missing signal reweighting.</done>
</task>

<task type="auto">
  <name>Task 2: Create comprehensive quality analysis unit tests</name>
  <files>tests/test_quality.py</files>
  <action>
Create `tests/test_quality.py` with comprehensive tests for the composite quality system. Use pytest.

Test categories:

1. **Composite scoring basics:**
   - Clean English text scores above threshold
   - Garbled text scores below threshold and is flagged
   - Mixed text (some garbled words) gets intermediate score
   - Empty/short text returns neutral result

2. **Signal breakdown:**
   - QualityResult.signal_scores contains "garbled" and "dictionary" keys
   - When confidence_data provided, signal_scores also contains "confidence"
   - Individual signal scores are 0-1 range

3. **Confidence signal integration:**
   - High confidence data boosts composite score
   - Low confidence data lowers composite score
   - Missing confidence data (None) gracefully handled — score uses garbled + dictionary only
   - Empty confidence data list handled (neutral 0.5)

4. **Dictionary signal:**
   - Common English words score high
   - Gibberish words score low
   - Academic terms (philosophy, scholarly) recognized

5. **German support:**
   - German philosophical terms not flagged as garbled
   - German compound words with valid suffixes (-keit, -heit, -ung, -schaft) not falsely flagged
   - Mixed German-English text handled correctly

6. **Signal floors:**
   - When any signal is below its floor, page is flagged even if composite is above threshold
   - Custom signal_floors override defaults

7. **Gray zone:**
   - Scores near threshold (within GRAY_ZONE) can be identified

8. **Backward compatibility:**
   - analyze() returns QualityResult with garbled_count, total_words, sample_issues
   - analyze_pages() returns list of QualityResult
   - get_bad_pages() returns list of int indices

9. **Edge cases:**
   - All punctuation text
   - Single word
   - Very long text (1000+ words)
   - Non-ASCII text (accented characters, Greek)
   - Text with only numbers/references

Target: 20+ test functions covering above categories.
  </action>
  <verify>
`pytest tests/test_quality.py -v` passes all tests.
`pytest tests/test_quality.py --tb=short` shows no failures.
  </verify>
  <done>Comprehensive unit tests cover composite scoring, individual signals, German support, signal floors, gray zone, backward compatibility, and edge cases. All tests pass.</done>
</task>

</tasks>

<verification>
- `pytest tests/test_quality.py -v` — all tests pass
- `ruff check src/scholardoc_ocr/quality.py` — no lint errors
- `python -c "from scholardoc_ocr.quality import QualityAnalyzer; q = QualityAnalyzer(languages=['en', 'de']); r = q.analyze('text'); print(r.signal_scores)"` — shows signal breakdown
- Composite score for clean text > 0.85
- Composite score for garbled text < 0.5
- German text not falsely flagged
</verification>

<success_criteria>
Composite quality analyzer integrates all signals with weighted scoring, per-signal breakdown in results, signal floors, gray zone detection, and German language support. Comprehensive test suite passes. Backward compatible API preserved.
</success_criteria>

<output>
After completion, create `.planning/phases/02-quality-analysis-enhancement/02-03-SUMMARY.md`
</output>
