---
phase: 08-robustness
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/scholardoc_ocr/logging_.py
autonomous: true

must_haves:
  truths:
    - "Worker process log messages arrive at the main process console on macOS"
    - "Per-worker log files are written to output_dir/logs/ with PID prefix"
    - "QueueListener is stopped in a finally block so logs are never lost"
  artifacts:
    - path: "src/scholardoc_ocr/logging_.py"
      provides: "QueueHandler/QueueListener setup, worker initializer, per-worker file handlers"
      contains: "QueueHandler"
  key_links:
    - from: "src/scholardoc_ocr/logging_.py"
      to: "logging.handlers.QueueHandler"
      via: "stdlib import"
      pattern: "QueueHandler"
---

<objective>
Create the multiprocess logging module that bridges worker process logs to the main process using QueueHandler/QueueListener, and writes per-worker log files.

Purpose: ROBU-01 and ROBU-07 — worker logs must reach the console on macOS (where fork+logging is broken) and per-worker log files must be available for post-run debugging.
Output: `src/scholardoc_ocr/logging_.py` with public API for pipeline integration.
</objective>

<execution_context>
@/Users/rookslog/.claude/get-shit-done/workflows/execute-plan.md
@/Users/rookslog/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-robustness/08-RESEARCH.md
@src/scholardoc_ocr/pipeline.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create logging_.py with QueueHandler/QueueListener infrastructure</name>
  <files>src/scholardoc_ocr/logging_.py</files>
  <action>
Create `src/scholardoc_ocr/logging_.py` with these public functions:

1. `setup_main_logging(log_dir: Path | None = None, verbose: bool = False) -> tuple[mp.Queue, QueueListener]`
   - Creates a `multiprocessing.Queue` for log transport
   - Creates a `QueueListener` that dispatches to:
     (a) A `StreamHandler` for console output (format: `%(asctime)s | %(levelname)s | %(name)s | %(message)s`)
     (b) If `log_dir` provided: a `RotatingFileHandler` writing to `log_dir/pipeline.log`
   - Starts the listener
   - Returns (queue, listener)
   - Sets root logger level to DEBUG if verbose, else INFO

2. `worker_log_initializer(log_queue: mp.Queue, log_dir: Path | None = None) -> None`
   - Intended as `ProcessPoolExecutor(initializer=...)` target
   - Clears all existing handlers on root logger
   - Adds a `QueueHandler(log_queue)` to root logger
   - If `log_dir` provided: also adds a `FileHandler` writing to `log_dir/worker_{os.getpid()}.log`
   - Sets root logger level to DEBUG

3. `stop_logging(listener: QueueListener) -> None`
   - Calls `listener.stop()` safely (no-op if already stopped)

Use `multiprocessing.Queue` (not `queue.Queue`) for fork-safety on macOS.
Do NOT use `logging.handlers.QueueHandler` from stdlib if Python < 3.12 has issues — check compatibility. Actually, QueueHandler has been in stdlib since 3.2, it's fine.

Include module docstring explaining the macOS fork+logging problem and why QueueHandler is needed.
  </action>
  <verify>
    `python -c "from scholardoc_ocr.logging_ import setup_main_logging, worker_log_initializer, stop_logging; print('OK')"`
  </verify>
  <done>Module imports cleanly, exposes three public functions for multiprocess logging setup.</done>
</task>

<task type="auto">
  <name>Task 2: Add unit tests for logging infrastructure</name>
  <files>tests/test_logging.py</files>
  <action>
Create `tests/test_logging.py` with tests:

1. `test_setup_main_logging_returns_queue_and_listener` — call `setup_main_logging()`, verify returns (Queue, QueueListener), then stop.
2. `test_worker_log_initializer_adds_queue_handler` — call `worker_log_initializer(queue)`, check root logger has a QueueHandler.
3. `test_worker_logs_reach_main_process` — full integration: setup main logging with a StringIO-based handler, run `worker_log_initializer` in a `ProcessPoolExecutor`, log a message from the worker, verify it appears in the main process output. Use `time.sleep(0.5)` to allow queue drain.
4. `test_per_worker_log_file_created` — call `worker_log_initializer(queue, log_dir=tmp_path)`, log a message, verify `worker_{pid}.log` exists in tmp_path.
5. `test_stop_logging_idempotent` — call `stop_logging` twice without error.

Use `pytest` and `tmp_path` fixture. Import from `scholardoc_ocr.logging_`.
  </action>
  <verify>`pytest tests/test_logging.py -v`</verify>
  <done>All 5 tests pass, confirming QueueHandler/QueueListener works across processes on macOS.</done>
</task>

</tasks>

<verification>
- `ruff check src/scholardoc_ocr/logging_.py` passes
- `pytest tests/test_logging.py -v` — all tests pass
- Manual: `python -c "from scholardoc_ocr.logging_ import setup_main_logging"` works
</verification>

<success_criteria>
- logging_.py exists with setup_main_logging, worker_log_initializer, stop_logging
- Worker process logs cross process boundary via QueueHandler on macOS
- Per-worker log files written with PID prefix
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/08-robustness/08-01-SUMMARY.md`
</output>
